{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/AshwinDeshpande96/Measuring_the_Impact_of_Verbal_Disfluency_Tags_on_Automated_Dementia_Detection/blob/master/FineTuneLOOCV_2020_GOLD_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments: Manually Annotated Transcripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H5eq2FbbxUkH"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import re\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.utils.data import TensorDataset, random_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from google.colab import drive\n",
    "import datetime\n",
    "from transformers import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "import time\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "from sklearn.model_selection import KFold\n",
    "import pickle\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "# drive.mount('/content/drive')\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### Global Variables ####################\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "UNUSED_TOKEN = {1: '[unused0]',\n",
    "                2: '[unused1]',\n",
    "                3: '[unused2]'}\n",
    "to_categorical = {'Control': 0,\n",
    "                  'Dementia': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################### Train and Test Sets ####################\n",
    "\n",
    "# train_text, temp_text, train_labels, temp_labels = train_test_split(df['transcript'], df['dx'],\n",
    "#                                                                     random_state=seed_val,\n",
    "#                                                                     test_size=0.3,\n",
    "#                                                                     stratify=df['dx']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "\n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#################### BERT features ####################\n",
    "\n",
    "def get_bert_tokenizer():\n",
    "    \n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_basic_tokenize=False)\n",
    "    tokenizer.add_special_tokens({ \"additional_special_tokens\": [ \"[unused0]\" ] })\n",
    "    return tokenizer\n",
    "\n",
    "def get_roberta_tokenizer():\n",
    "    from transformers import RobertaTokenizer\n",
    "    tokenizer = RobertaTokenizer.from_pretrained('roberta-base', do_basic_tokenize=False)\n",
    "    tokenizer.add_special_tokens({ \"additional_special_tokens\": [ \"[unused0]\" ] })\n",
    "    return tokenizer\n",
    "\n",
    "def get_distilbert_tokenizer():\n",
    "    from transformers import DistilBertTokenizer\n",
    "    tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased', do_basic_tokenize=False)\n",
    "    tokenizer.add_special_tokens({ \"additional_special_tokens\": [ \"[unused0]\" ] })\n",
    "    return tokenizer\n",
    "\n",
    "def encode_sentence(transcript, tokenizer):\n",
    "    transcript = re.sub(r\"\\.\\s\", \" [SEP]\", transcript)\n",
    "    tokens = list(tokenizer.tokenize(transcript))\n",
    "    tokens = ['[CLS]'] + tokens + ['[SEP]']\n",
    "    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    return token_ids\n",
    "\n",
    "\n",
    "def add_padding(input_word_ids, max_seq_len):\n",
    "    input_type = []\n",
    "    for idx, embedding in enumerate(input_word_ids):\n",
    "        embedding_len = len(embedding)\n",
    "        e_input_type = np.ones(embedding_len, dtype=np.int64).tolist()\n",
    "        if embedding_len < max_seq_len:\n",
    "            zeros = np.zeros(max_seq_len - embedding_len, dtype=np.int64).tolist()\n",
    "            e_input_type += zeros\n",
    "            embedding += zeros\n",
    "        elif embedding_len > max_seq_len:\n",
    "            embedding = embedding[:max_seq_len - 1] + [102]\n",
    "            e_input_type = e_input_type[:max_seq_len]\n",
    "        input_type.append(torch.tensor([e_input_type]))\n",
    "        input_word_ids[idx] = torch.tensor([embedding])\n",
    "    return {'input_ids': input_word_ids, 'attention_mask': input_type}\n",
    "\n",
    "\n",
    "def bert_encode(transcripts, tokenizer, max_seq_len):\n",
    "    input_word_ids = [encode_sentence(s, tokenizer)\n",
    "                      for s in transcripts]\n",
    "    input_word_ids = add_padding(input_word_ids, max_seq_len)\n",
    "    return input_word_ids\n",
    "\n",
    "############################# Models ##################################\n",
    "def get_bert_model():\n",
    "    from transformers import BertForSequenceClassification\n",
    "    model = BertForSequenceClassification.from_pretrained(\n",
    "        \"bert-base-uncased\",  # Use the 12-layer BERT model, with an uncased vocab.\n",
    "        num_labels=2,  # The number of output labels--2 for binary classification.\n",
    "        # You can increase this for multi-class tasks.\n",
    "        output_attentions=False,  # Whether the model returns attentions weights.\n",
    "        output_hidden_states=False,  # Whether the model returns all hidden-states.\n",
    "    )\n",
    "    model.cuda()\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_roberta_model():\n",
    "    from transformers import RobertaForSequenceClassification\n",
    "    model = RobertaForSequenceClassification.from_pretrained(\n",
    "        \"roberta-base\",  # Use the 12-layer BERT model, with an uncased vocab.\n",
    "        num_labels=2,  # The number of output labels--2 for binary classification.\n",
    "        # You can increase this for multi-class tasks.\n",
    "        output_attentions=False,  # Whether the model returns attentions weights.\n",
    "        output_hidden_states=False,  # Whether the model returns all hidden-states.\n",
    "    )\n",
    "    model.cuda()\n",
    "    return model\n",
    "\n",
    "def get_distilbert_model():\n",
    "    from transformers import DistilBertForSequenceClassification\n",
    "    model = DistilBertForSequenceClassification.from_pretrained(\n",
    "        \"distilbert-base-uncased\",  # Use the 12-layer BERT model, with an uncased vocab.\n",
    "        num_labels=2,  # The number of output labels--2 for binary classification.\n",
    "        # You can increase this for multi-class tasks.\n",
    "        output_attentions=False,  # Whether the model returns attentions weights.\n",
    "        output_hidden_states=False,  # Whether the model returns all hidden-states.\n",
    "    )\n",
    "    model.cuda()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2Xvqyf2UFeKu"
   },
   "outputs": [],
   "source": [
    "def reps():\n",
    "    df = pd.read_pickle('data2020fisher_reps.pickle')\n",
    "    X1 = df.transcript_without_tags.to_numpy()\n",
    "    X2 = df.transcript_with_stopword_rep.to_numpy()\n",
    "    X3 = df.transcript_with_non_stopword_rep.to_numpy()\n",
    "    X4 = df.transcript_with_no_rep.to_numpy()\n",
    "    X5 = df.transcript_with_no_filled_pauses.to_numpy()\n",
    "    experiments = ['transcript_without_tags',\n",
    "                  'transcript_with_stopword_rep',\n",
    "                  'transcript_with_non_stopword_rep',\n",
    "                  'transcript_with_no_rep',\n",
    "                  'transcript_with_no_filled_pauses']\n",
    "    Xs = [X1, X2, X3, X4, X5]\n",
    "    df.dx = df.dx.apply(lambda x: to_categorical[x])\n",
    "    y = df.dx.to_numpy()\n",
    "    return df, experiments, Xs, y\n",
    "\n",
    "def all_exp():\n",
    "    df = pd.read_pickle('data2020fisher_all.pickle')\n",
    "    experiments = ['transcript_without_tags',\n",
    "                 'transcript_without_underscore',\n",
    "                 'transcript_without_repetition',\n",
    "                 'transcript_without_retracing',\n",
    "                 'transcript_without_disfluency',\n",
    "                ]\n",
    "    X1 = df.transcript_without_tags.to_numpy()\n",
    "    X2 = df.transcript_without_underscore.to_numpy()\n",
    "    X3 = df.transcript_without_repetition.to_numpy()\n",
    "    X4 = df.transcript_without_retracing.to_numpy()\n",
    "    X5 = df.transcript_without_disfluency.to_numpy()\n",
    "    Xs = [X1, X2, X3, X4, X5]\n",
    "    df.dx = df.dx.apply(lambda x: to_categorical[x])\n",
    "    y = df.dx.to_numpy()\n",
    "    return df, experiments, Xs, y\n",
    "def new_data():\n",
    "    df = pd.read_pickle('new_data.pickle').dropna()\n",
    "    df = df[df.both_eremoved != \"placeholder\"]\n",
    "    experiments = ['all_errors', \"e_tagged\", \"rep_eremoved\", \"ret_eremoved\", \"both_eremoved\"]\n",
    "    df.dx = df.dx.apply(lambda x: to_categorical[x])\n",
    "    X1 = df.all_errors.to_numpy()\n",
    "    X2 = df.e_tagged.to_numpy()\n",
    "    X3 = df.rep_eremoved.to_numpy()\n",
    "    X4 = df.ret_eremoved.to_numpy()\n",
    "    X5 = df.both_eremoved.to_numpy()\n",
    "    Xs = [X1, X2, X3, X4, X5]\n",
    "    y = df.dx.to_numpy()\n",
    "    return df, experiments, Xs, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "nMLC4h8xCpLj",
    "outputId": "e5522209-b3d6-40bb-9a23-d1211c23ce33",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1138, 15)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>utt_id</th>\n",
       "      <th>dx</th>\n",
       "      <th>mmse</th>\n",
       "      <th>disfluency_asr</th>\n",
       "      <th>transcript_with_tags</th>\n",
       "      <th>transcript_tokens</th>\n",
       "      <th>both_errors</th>\n",
       "      <th>both_etype</th>\n",
       "      <th>both_reference_text</th>\n",
       "      <th>asr_text_map</th>\n",
       "      <th>rep_eremoved</th>\n",
       "      <th>all_errors</th>\n",
       "      <th>e_tagged</th>\n",
       "      <th>ret_eremoved</th>\n",
       "      <th>both_eremoved</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>S051_0_2560</td>\n",
       "      <td>0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>we _ 'll _ start _ with _ a _ girl _</td>\n",
       "      <td>*par we 'll start with the girl . \u00150 2560\u0015</td>\n",
       "      <td>[*par, we, 'll, start, with, the, girl, ., \u00150,...</td>\n",
       "      <td>{'[/]': [], '[//]': []}</td>\n",
       "      <td>{'we_1': None, ''ll_1': None, 'start_1': None,...</td>\n",
       "      <td>we 'll start with the girl</td>\n",
       "      <td>[(we, _, we, None), ('ll, _, 'll, None), (star...</td>\n",
       "      <td>we 'll start with a girl</td>\n",
       "      <td>we 'll start with a girl</td>\n",
       "      <td>we 'll start with a girl</td>\n",
       "      <td>we 'll start with a girl</td>\n",
       "      <td>we 'll start with a girl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>855</th>\n",
       "      <td>S087_65133_74985</td>\n",
       "      <td>1</td>\n",
       "      <td>7.0</td>\n",
       "      <td>that _ 's _ all _ there _ is _</td>\n",
       "      <td>*par (.) the father is n't comin(g) . \u001565133 7...</td>\n",
       "      <td>[*par, (.), the, father, is, n't, comin(g), .,...</td>\n",
       "      <td>{'[/]': [], '[//]': []}</td>\n",
       "      <td>{'the_1': None, 'father_1': None, 'is_1': None...</td>\n",
       "      <td>the father is n't coming</td>\n",
       "      <td>[(that, _, the, None), ('s, _, father, None), ...</td>\n",
       "      <td>that 's all there is</td>\n",
       "      <td>that 's all there is</td>\n",
       "      <td>that 's all there is</td>\n",
       "      <td>that 's all there is</td>\n",
       "      <td>that 's all there is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>646</th>\n",
       "      <td>S064_22479_25655</td>\n",
       "      <td>0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>and _ the _ windows _ up _</td>\n",
       "      <td>*par and &amp;uh the window 's up . \u001522479 25655\u0015</td>\n",
       "      <td>[*par, and, &amp;uh, the, window, 's, up, ., \u00152247...</td>\n",
       "      <td>{'[/]': [], '[//]': []}</td>\n",
       "      <td>{'and_1': None, 'uh_1': None, 'the_1': None, '...</td>\n",
       "      <td>and uh the window 's up</td>\n",
       "      <td>[(and, _, and, None), (**, None, uh, None), (t...</td>\n",
       "      <td>and the windows up</td>\n",
       "      <td>and the windows up</td>\n",
       "      <td>and the windows up</td>\n",
       "      <td>and the windows up</td>\n",
       "      <td>and the windows up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1095</th>\n",
       "      <td>S108_100964_107229</td>\n",
       "      <td>1</td>\n",
       "      <td>13.0</td>\n",
       "      <td>existing _ here _ is _ turning _ over _</td>\n",
       "      <td>*par well this thing here is &amp;uh turnin(g) ove...</td>\n",
       "      <td>[*par, well, this, thing, here, is, &amp;uh, turni...</td>\n",
       "      <td>{'[/]': [], '[//]': []}</td>\n",
       "      <td>{'well_1': None, 'this_1': None, 'thing_1': No...</td>\n",
       "      <td>well this thing here is uh turning over</td>\n",
       "      <td>[(****, None, well, None), (****, None, this, ...</td>\n",
       "      <td>existing here is turning over</td>\n",
       "      <td>existing here is turning over</td>\n",
       "      <td>existing here is turning over</td>\n",
       "      <td>existing here is turning over</td>\n",
       "      <td>existing here is turning over</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>426</th>\n",
       "      <td>S039_36199_38888</td>\n",
       "      <td>0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>and _ the _ sink _ is _ running _</td>\n",
       "      <td>*par and the &amp;um &amp;uh sink is running over . \u00153...</td>\n",
       "      <td>[*par, and, the, &amp;um, &amp;uh, sink, is, running, ...</td>\n",
       "      <td>{'[/]': [], '[//]': []}</td>\n",
       "      <td>{'and_1': None, 'the_1': None, 'um_1': None, '...</td>\n",
       "      <td>and the um uh sink is running over</td>\n",
       "      <td>[(and, _, and, None), (the, _, the, None), (**...</td>\n",
       "      <td>and the sink is running</td>\n",
       "      <td>and the sink is running</td>\n",
       "      <td>and the sink is running</td>\n",
       "      <td>and the sink is running</td>\n",
       "      <td>and the sink is running</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1135</th>\n",
       "      <td>S110_21121_23565</td>\n",
       "      <td>1</td>\n",
       "      <td>16.0</td>\n",
       "      <td>and _ a _ stool _ is _ turned _ over _</td>\n",
       "      <td>*par and a stool is turned over . \u001521121 23565\u0015</td>\n",
       "      <td>[*par, and, a, stool, is, turned, over, ., \u001521...</td>\n",
       "      <td>{'[/]': [], '[//]': []}</td>\n",
       "      <td>{'and_1': None, 'a_1': None, 'stool_1': None, ...</td>\n",
       "      <td>and a stool is turned over</td>\n",
       "      <td>[(and, _, and, None), (a, _, a, None), (stool,...</td>\n",
       "      <td>and a stool is turned over</td>\n",
       "      <td>and a stool is turned over</td>\n",
       "      <td>and a stool is turned over</td>\n",
       "      <td>and a stool is turned over</td>\n",
       "      <td>and a stool is turned over</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449</th>\n",
       "      <td>S041_22863_30896</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>and _ her _ right _ hand _ looks _ like _ she ...</td>\n",
       "      <td>*par and her right hand looks like she 's almo...</td>\n",
       "      <td>[*par, and, her, right, hand, looks, like, she...</td>\n",
       "      <td>{'[/]': [], '[//]': []}</td>\n",
       "      <td>{'and_1': None, 'her_1': None, 'right_1': None...</td>\n",
       "      <td>and her right hand looks like she 's almost uh...</td>\n",
       "      <td>[(and, _, and, None), (her, _, her, None), (ri...</td>\n",
       "      <td>and her right hand looks like she 's almost tr...</td>\n",
       "      <td>and her right hand looks like she 's almost tr...</td>\n",
       "      <td>and her right hand looks like she 's almost tr...</td>\n",
       "      <td>and her right hand looks like she 's almost tr...</td>\n",
       "      <td>and her right hand looks like she 's almost tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>908</th>\n",
       "      <td>S092_51969_53216</td>\n",
       "      <td>1</td>\n",
       "      <td>15.0</td>\n",
       "      <td>no _ that _ 's _ all _</td>\n",
       "      <td>*par no that 's all . [ exc] \u001551969 53216\u0015</td>\n",
       "      <td>[*par, no, that, 's, all, ., [, exc], \u001551969, ...</td>\n",
       "      <td>{'[/]': [], '[//]': []}</td>\n",
       "      <td>{'no_1': None, 'that_1': None, ''s_1': None, '...</td>\n",
       "      <td>no that 's all</td>\n",
       "      <td>[(no, _, no, None), (that, _, that, None), ('s...</td>\n",
       "      <td>no that 's all</td>\n",
       "      <td>no that 's all</td>\n",
       "      <td>no that 's all</td>\n",
       "      <td>no that 's all</td>\n",
       "      <td>no that 's all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>S013_22022_25481</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>you _ want _ like _ the _ windows _ open _ tha...</td>\n",
       "      <td>*par &amp;uh do [/] do you want like the window 's...</td>\n",
       "      <td>[*par, &amp;uh, DOREP, [/], do, you, want, like, t...</td>\n",
       "      <td>{'[/]': [[2]], '[//]': []}</td>\n",
       "      <td>{'uh_1': None, 'DO_1': 'REP', 'do_1': None, 'y...</td>\n",
       "      <td>uh DO do you want like the window 's open that...</td>\n",
       "      <td>[(**, None, uh, None), (**, None, DO, REP), (*...</td>\n",
       "      <td>you want like the windows open that sort of thing</td>\n",
       "      <td>you want like the windows open that sort of thing</td>\n",
       "      <td>you want like the windows open that sort of thing</td>\n",
       "      <td>you want like the windows open that sort of thing</td>\n",
       "      <td>you want like the windows open that sort of thing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1394</th>\n",
       "      <td>S140_63734_66044</td>\n",
       "      <td>1</td>\n",
       "      <td>17.0</td>\n",
       "      <td>selected _ rock _</td>\n",
       "      <td>*par and she must have dropped one . \u001563734 66...</td>\n",
       "      <td>[*par, and, she, must, have, dropped, one, ., ...</td>\n",
       "      <td>{'[/]': [], '[//]': []}</td>\n",
       "      <td>{'and_1': None, 'she_1': None, 'must_1': None,...</td>\n",
       "      <td>and she must have dropped one</td>\n",
       "      <td>[(***, None, and, None), (***, None, she, None...</td>\n",
       "      <td>selected rock</td>\n",
       "      <td>selected rock</td>\n",
       "      <td>selected rock</td>\n",
       "      <td>selected rock</td>\n",
       "      <td>selected rock</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  utt_id  ...                                      both_eremoved\n",
       "501          S051_0_2560  ...                           we 'll start with a girl\n",
       "855     S087_65133_74985  ...                               that 's all there is\n",
       "646     S064_22479_25655  ...                                 and the windows up\n",
       "1095  S108_100964_107229  ...                      existing here is turning over\n",
       "426     S039_36199_38888  ...                            and the sink is running\n",
       "1135    S110_21121_23565  ...                         and a stool is turned over\n",
       "449     S041_22863_30896  ...  and her right hand looks like she 's almost tr...\n",
       "908     S092_51969_53216  ...                                     no that 's all\n",
       "161     S013_22022_25481  ...  you want like the windows open that sort of thing\n",
       "1394    S140_63734_66044  ...                                      selected rock\n",
       "\n",
       "[10 rows x 15 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df, experiments, Xs, y = new_data()\n",
    "print(df.shape)\n",
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"bert\"\n",
    "lr = 4e-5\n",
    "max_seq_len = 256\n",
    "epochs = 8\n",
    "# epochs = 1\n",
    "batch_size_tr = 16\n",
    "batch_size_ts = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_dict = OrderedDict(zip(experiments, [[] for _ in range(len(experiments))]))\n",
    "f1_dict = OrderedDict(zip(experiments, [[] for _ in range(len(experiments))]))\n",
    "# class1_dict_acc = OrderedDict(zip(experiments, [[] for _ in range(len(experiments))]))\n",
    "class1_dict_f1 = OrderedDict(zip(experiments, [[] for _ in range(len(experiments))]))\n",
    "class1_dict_pr = OrderedDict(zip(experiments, [[] for _ in range(len(experiments))]))\n",
    "class1_dict_re = OrderedDict(zip(experiments, [[] for _ in range(len(experiments))]))\n",
    "\n",
    "# class0_dict_acc = OrderedDict(zip(experiments, [[] for _ in range(len(experiments))]))\n",
    "class0_dict_f1 = OrderedDict(zip(experiments, [[] for _ in range(len(experiments))]))\n",
    "class0_dict_pr = OrderedDict(zip(experiments, [[] for _ in range(len(experiments))]))\n",
    "class0_dict_re = OrderedDict(zip(experiments, [[] for _ in range(len(experiments))]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments\n",
    " \n",
    "Here we set up experiments and measure performance by varying the input X\n",
    "* with all errors\n",
    "* without repetition error\n",
    "* without retracing error \n",
    "* without any error\n",
    "* other\n",
    "\n",
    "and keeping the output variable Y constant\n",
    "\n",
    "We expect the performance to degrade as follows\n",
    "\n",
    "with all errors > without repetition error, without retracing error > without any error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PbF4kWxnIbU6",
    "outputId": "fcf22770-cb71-48b1-af96-74139a78cf77",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "loo = LeaveOneOut()\n",
    "# print(f\"SEED: {seed_val}\\nMODEL: {model_name}\\nBATCH SIZE TR: {batch_size_tr}\\nBATCH SIZE TS: {batch_size_ts}\\nLEARNING RATE: {lr}\\nEMBEDDING LEN: {max_seq_len}\\nEPOCH: {epochs}\")\n",
    "# Seed Value is changed and averaged since BERT based models are sensitive to initial conditions\n",
    "for seed_val in [0, 42, 23, 17, 19, 111]:\n",
    "    for name, X in zip(experiments, Xs):\n",
    "        # For each experiment\n",
    "        print(\"#\"*10, name)\n",
    "        # Reset Seeds\n",
    "        random.seed(seed_val)\n",
    "        np.random.seed(seed_val)\n",
    "        torch.manual_seed(seed_val)\n",
    "        torch.cuda.manual_seed_all(seed_val)\n",
    "        \n",
    "        # Set up KFOLD CV along\n",
    "        kf = KFold(n_splits=5, random_state=seed_val, shuffle=True)\n",
    "        \n",
    "        # Performance metric for an experiment\n",
    "        total_preds = []\n",
    "        total_target = []\n",
    "        f1_set = []\n",
    "        acc_set = []\n",
    "        fold=1\n",
    "\n",
    "        for train_index, test_index in kf.split(X):\n",
    "            print('fold %d'%(fold))\n",
    "            random.seed(seed_val)\n",
    "            np.random.seed(seed_val)\n",
    "            torch.manual_seed(seed_val)\n",
    "            torch.cuda.manual_seed_all(seed_val)\n",
    "            \n",
    "            # Fetch Model and Tokenizer according to chosen Model\n",
    "            tokenizer = locals()[f\"get_{model_name.lower()}_tokenizer\"]()\n",
    "            model = locals()[f\"get_{model_name.lower()}_model\"]()\n",
    "            \n",
    "            # Prepare Train and Test Data\n",
    "            train_text, temp_text = X[train_index], X[test_index]\n",
    "            train_labels, temp_labels = y[train_index], y[test_index]\n",
    "            fold+=1\n",
    "            \n",
    "            tokens_train = bert_encode(train_text.tolist(), tokenizer, max_seq_len)\n",
    "            tokens_test = bert_encode(temp_text.tolist(), tokenizer, max_seq_len)\n",
    "\n",
    "            #################### Torch Dataset ####################\n",
    "\n",
    "            input_ids = torch.cat(tokens_train['input_ids'], dim=0)\n",
    "            attention_masks = torch.cat(tokens_train['attention_mask'], dim=0)\n",
    "            labels = torch.tensor(train_labels.tolist())\n",
    "\n",
    "            input_ids_test = torch.cat(tokens_test['input_ids'], dim=0)\n",
    "            attention_masks_test = torch.cat(tokens_test['attention_mask'], dim=0)\n",
    "            labels_test = torch.tensor(temp_labels.tolist())\n",
    "\n",
    "            #################### Validation Dataset ##############\n",
    "            dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "            train_size = int(0.9 * len(dataset))\n",
    "            val_size = len(dataset) - train_size\n",
    "            train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "            \n",
    "            # Prepare Train and Validation dataset from Train\n",
    "            train_dataloader = DataLoader(\n",
    "                train_dataset,  # The training samples.\n",
    "                sampler=RandomSampler(train_dataset),  # Select batches randomly\n",
    "                batch_size=batch_size_tr  # Trains with this batch size.\n",
    "            )\n",
    "\n",
    "            validation_dataloader = DataLoader(\n",
    "                val_dataset,  # The validation samples.\n",
    "                sampler=SequentialSampler(val_dataset),  # Pull out batches sequentially.\n",
    "                batch_size=batch_size_tr  # Evaluate with this batch size.\n",
    "            )\n",
    "            \n",
    "            ############################# Training ################################\n",
    "\n",
    "            total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "            optimizer = AdamW(model.parameters(),\n",
    "                            lr=lr,  # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "                            eps=1e-8  # args.adam_epsilon  - default is 1e-8.\n",
    "                            )\n",
    "\n",
    "            scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                                        num_warmup_steps=0,  # Default value in run_glue.py\n",
    "                                                        num_training_steps=total_steps)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            training_stats = []\n",
    "\n",
    "            # Measure the total training time for the whole run.\n",
    "            total_t0 = time.time()\n",
    "\n",
    "            # For each epoch...\n",
    "            for epoch_i in range(0, epochs):\n",
    "\n",
    "                # ========================================\n",
    "                #               Training\n",
    "                # ========================================\n",
    "\n",
    "                # Perform one full pass over the training set.\n",
    "\n",
    "                # print(\"\")\n",
    "                # print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "                # print('Training...')\n",
    "\n",
    "                # Measure how long the training epoch takes.\n",
    "                t0 = time.time()\n",
    "\n",
    "                # Reset the total loss for this epoch.\n",
    "                total_train_loss = 0\n",
    "\n",
    "                # Put the model into training mode. Don't be mislead--the call to\n",
    "                # `train` just changes the *mode*, it doesn't *perform* the training.\n",
    "                # `dropout` and `batchnorm` layers behave differently during training\n",
    "                # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
    "                model.train()\n",
    "\n",
    "                # For each batch of training data...\n",
    "                for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "                    # Progress update every 40 batches.\n",
    "                    if step % 40 == 0 and not step == 0:\n",
    "                        # Calculate elapsed time in minutes.\n",
    "                        elapsed = format_time(time.time() - t0)\n",
    "\n",
    "                        # Report progress.\n",
    "                        # print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "                    # Unpack this training batch from our dataloader.\n",
    "                    #\n",
    "                    # As we unpack the batch, we'll also copy each tensor to the GPU using the\n",
    "                    # `to` method.\n",
    "                    #\n",
    "                    # `batch` contains three pytorch tensors:\n",
    "                    #   [0]: input ids\n",
    "                    #   [1]: attention masks\n",
    "                    #   [2]: labels\n",
    "                    b_input_ids = batch[0].to(device)\n",
    "                    b_input_mask = batch[1].to(device)\n",
    "                    b_labels = batch[2].to(device)\n",
    "\n",
    "                    # Always clear any previously calculated gradients before performing a\n",
    "                    # backward pass. PyTorch doesn't do this automatically because\n",
    "                    # accumulating the gradients is \"convenient while training RNNs\".\n",
    "                    # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
    "                    model.zero_grad()\n",
    "\n",
    "                    # Perform a forward pass (evaluate the model on this training batch).\n",
    "                    # In PyTorch, calling `model` will in turn call the model's `forward`\n",
    "                    # function and pass down the arguments. The `forward` function is\n",
    "                    # documented here:\n",
    "                    # https://huggingface.co/transformers/model_doc/bert.html#bertforsequenceclassification\n",
    "                    # The results are returned in a results object, documented here:\n",
    "                    # https://huggingface.co/transformers/main_classes/output.html#transformers.modeling_outputs.SequenceClassifierOutput\n",
    "                    # Specifically, we'll get the loss (because we provided labels) and the\n",
    "                    # \"logits\"--the model outputs prior to activation.\n",
    "                    try:\n",
    "                        result = model(b_input_ids,\n",
    "                                    token_type_ids=None,\n",
    "                                    attention_mask=b_input_mask,\n",
    "                                    labels=b_labels,\n",
    "                                    return_dict=True)\n",
    "                    except:\n",
    "                        result = model(b_input_ids,\n",
    "                                        #    token_type_ids=None,\n",
    "                                        attention_mask=b_input_mask,\n",
    "                                        labels=b_labels,\n",
    "                                        return_dict=True)                    \n",
    "\n",
    "                    loss = result.loss\n",
    "                    logits = result.logits\n",
    "\n",
    "                    # Accumulate the training loss over all of the batches so that we can\n",
    "                    # calculate the average loss at the end. `loss` is a Tensor containing a\n",
    "                    # single value; the `.item()` function just returns the Python value\n",
    "                    # from the tensor.\n",
    "                    total_train_loss += loss.item()\n",
    "\n",
    "                    # Perform a backward pass to calculate the gradients.\n",
    "                    loss.backward()\n",
    "\n",
    "                    # Clip the norm of the gradients to 1.0.\n",
    "                    # This is to help prevent the \"exploding gradients\" problem.\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "                    # Update parameters and take a step using the computed gradient.\n",
    "                    # The optimizer dictates the \"update rule\"--how the parameters are\n",
    "                    # modified based on their gradients, the learning rate, etc.\n",
    "                    optimizer.step()\n",
    "\n",
    "                    # Update the learning rate.\n",
    "                    scheduler.step()\n",
    "\n",
    "                # Calculate the average loss over all of the batches.\n",
    "                avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "\n",
    "                # Measure how long this epoch took.\n",
    "                training_time = format_time(time.time() - t0)\n",
    "\n",
    "                # print(\"\")\n",
    "                # print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "                # print(\"  Training epcoh took: {:}\".format(training_time))\n",
    "\n",
    "                # ========================================\n",
    "                #               Validation\n",
    "                # ========================================\n",
    "                # After the completion of each training epoch, measure our performance on\n",
    "                # our validation set.\n",
    "\n",
    "                # print(\"\")\n",
    "                # print(\"Running Validation...\")\n",
    "\n",
    "                t0 = time.time()\n",
    "\n",
    "                # Put the model in evaluation mode--the dropout layers behave differently\n",
    "                # during evaluation.\n",
    "                model.eval()\n",
    "\n",
    "                # Tracking variables\n",
    "                total_eval_accuracy = 0\n",
    "                total_eval_loss = 0\n",
    "                nb_eval_steps = 0\n",
    "\n",
    "                # Evaluate data for one epoch\n",
    "                for batch in validation_dataloader:\n",
    "                    # Unpack this training batch from our dataloader.\n",
    "                    #\n",
    "                    # As we unpack the batch, we'll also copy each tensor to the GPU using\n",
    "                    # the `to` method.\n",
    "                    #\n",
    "                    # `batch` contains three pytorch tensors:\n",
    "                    #   [0]: input ids\n",
    "                    #   [1]: attention masks\n",
    "                    #   [2]: labels\n",
    "                    b_input_ids = batch[0].to(device)\n",
    "                    b_input_mask = batch[1].to(device)\n",
    "                    b_labels = batch[2].to(device)\n",
    "\n",
    "                    # Tell pytorch not to bother with constructing the compute graph during\n",
    "                    # the forward pass, since this is only needed for backprop (training).\n",
    "                    with torch.no_grad():\n",
    "                        # Forward pass, calculate logit predictions.\n",
    "                        # token_type_ids is the same as the \"segment ids\", which\n",
    "                        # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
    "                        result = model(b_input_ids,\n",
    "                                    #    token_type_ids=None,\n",
    "                                    attention_mask=b_input_mask,\n",
    "                                    labels=b_labels,\n",
    "                                    return_dict=True)\n",
    "\n",
    "                    # Get the loss and \"logits\" output by the model. The \"logits\" are the\n",
    "                    # output values prior to applying an activation function like the\n",
    "                    # softmax.\n",
    "                    loss = result.loss\n",
    "                    logits = result.logits\n",
    "\n",
    "                    # Accumulate the validation loss.\n",
    "                    total_eval_loss += loss.item()\n",
    "\n",
    "                    # Move logits and labels to CPU\n",
    "                    logits = logits.detach().cpu().numpy()\n",
    "                    label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "                    # Calculate the accuracy for this batch of test sentences, and\n",
    "                    # accumulate it over all batches.\n",
    "                    total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "\n",
    "                # Report the final accuracy for this validation run.\n",
    "                avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "                # print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
    "\n",
    "                # Calculate the average loss over all of the batches.\n",
    "                avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "\n",
    "                # Measure how long the validation run took.\n",
    "                validation_time = format_time(time.time() - t0)\n",
    "\n",
    "                # print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "                # print(\"  Validation took: {:}\".format(validation_time))\n",
    "\n",
    "                # Record all statistics from this epoch.\n",
    "                training_stats.append(\n",
    "                    {\n",
    "                        'epoch': epoch_i + 1,\n",
    "                        'Training Loss': avg_train_loss,\n",
    "                        'Valid. Loss': avg_val_loss,\n",
    "                        'Valid. Accur.': avg_val_accuracy,\n",
    "                        'Training Time': training_time,\n",
    "                        'Validation Time': validation_time\n",
    "                    }\n",
    "                )\n",
    "\n",
    "            # print(\"\")\n",
    "            # print(\"Training complete!\")\n",
    "\n",
    "            # print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time() - total_t0)))\n",
    "\n",
    "            # Create the DataLoader.\n",
    "            prediction_data = TensorDataset(input_ids_test, attention_masks_test, labels_test)\n",
    "            prediction_sampler = SequentialSampler(prediction_data)\n",
    "            prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size_ts)\n",
    "            ################################################################################\n",
    "            # Prediction on test set\n",
    "\n",
    "            # print('Predicting labels for {:,} test sentences...'.format(len(input_ids)))\n",
    "\n",
    "            # Put model in evaluation mode\n",
    "            model.eval()\n",
    "\n",
    "            # Tracking variables \n",
    "            predictions , true_labels = [], []\n",
    "\n",
    "            # Predict \n",
    "            for batch in prediction_dataloader:\n",
    "                # Add batch to GPU\n",
    "                batch = tuple(t.to(device) for t in batch)\n",
    "            \n",
    "                # Unpack the inputs from our dataloader\n",
    "                b_input_ids, b_input_mask, b_labels = batch\n",
    "                \n",
    "                # Telling the model not to compute or store gradients, saving memory and \n",
    "                # speeding up prediction\n",
    "                with torch.no_grad():\n",
    "\n",
    "                    # Forward pass, calculate logit predictions.\n",
    "                    try:\n",
    "                        result = model(b_input_ids, \n",
    "                                        token_type_ids=None, \n",
    "                                        attention_mask=b_input_mask,\n",
    "                                        return_dict=True)\n",
    "                    except:\n",
    "                        result = model(b_input_ids, \n",
    "                                        #  token_type_ids=None, \n",
    "                                        attention_mask=b_input_mask,\n",
    "                                        return_dict=True)\n",
    "\n",
    "                logits = result.logits\n",
    "\n",
    "                # Move logits and labels to CPU\n",
    "                logits = logits.detach().cpu().numpy()\n",
    "                label_ids = b_labels.to('cpu').numpy()\n",
    "                \n",
    "                # Store predictions and true labels\n",
    "                predictions.append(logits)\n",
    "                true_labels.append(label_ids)\n",
    "\n",
    "            # print('    DONE.')\n",
    "            ######################### Save Performance for each Fold ############################\n",
    "            \n",
    "            \n",
    "\n",
    "            # Evaluate each test batch using Matthew's correlation coefficient\n",
    "            # print('Calculating Acc and F1 Corr. Coef. for each batch...')\n",
    "\n",
    "            # For each input batch...\n",
    "            for i in range(len(true_labels)):\n",
    "                # The predictions for this batch are a 2-column ndarray (one column for \"0\" \n",
    "                # and one column for \"1\"). Pick the label with the highest value and turn this\n",
    "                # in to a list of 0s and 1s.\n",
    "                pred_labels_i = np.argmax(predictions[i], axis=1).flatten()\n",
    "                total_target += true_labels[i].tolist()\n",
    "                total_preds += pred_labels_i.tolist()\n",
    "                \n",
    "                # Calculate and store the acc for this batch.\n",
    "                acc = accuracy_score(true_labels[i], pred_labels_i)          \n",
    "                acc_set.append(acc)\n",
    "\n",
    "                f1 = f1_score(true_labels[i], pred_labels_i)\n",
    "                # print(f\"Acc: {acc}\\tF1: {f1}\")          \n",
    "                f1_set.append(f1)\n",
    "                \n",
    "                \n",
    "        #######################  Get Average over all folds ##########################\n",
    "        avgacc = np.mean(acc_set)\n",
    "        accuracy_dict[name].append(avgacc)\n",
    "        avgf1 = np.mean(f1_set)\n",
    "        f1_dict[name].append(avgf1)\n",
    "        print(f\"AVG Acc: {avgacc}\\tAVG F1: {avgf1}\")\n",
    "        scores = precision_recall_fscore_support(total_target, total_preds, average=None, labels=[0,1])\n",
    "        pr0, pr1 = scores[0]\n",
    "        re0, re1 = scores[1]\n",
    "        f10, f11 = scores[2]\n",
    "        total_acc = accuracy_score(total_target, total_preds)\n",
    "        total_f1 = f1_score(total_target, total_preds)\n",
    "        class1_dict_f1[name].append(f11)\n",
    "        class1_dict_pr[name].append(pr1)\n",
    "        class1_dict_re[name].append(re1)\n",
    "\n",
    "        class0_dict_f1[name].append(f10)\n",
    "        class0_dict_pr[name].append(pr0)\n",
    "        class0_dict_re[name].append(re0)\n",
    "        \n",
    "        # print(f\"Total Acc: {total_acc}\\tTotal F1: {total_f1}\")\n",
    "        # print(confusion_matrix(total_target, total_preds))\n",
    "header = \"experiments,0,42,23,avg,std\\n\"\n",
    "output_acc = header[::]\n",
    "output_f1 = header[::]\n",
    "class1_acc = header[::]\n",
    "class1_f1 = header[::]\n",
    "class1_pr = header[::]\n",
    "class1_re = header[::]\n",
    "class0_acc = header[::]\n",
    "class0_f1 = header[::]\n",
    "class0_pr = header[::]\n",
    "class0_re = header[::]\n",
    "\n",
    "print(\"acc\")\n",
    "print(header)\n",
    "for key in accuracy_dict:\n",
    "    values = accuracy_dict[key]\n",
    "    values += [np.mean(values), np.std(values)]\n",
    "    values = \",\".join([str(elem) for elem in values])\n",
    "    output_acc += f\"{key},{values}\\n\"\n",
    "    print(key, \",\", values)\n",
    "print(\"\\n\\n\")\n",
    "print(\"f1\")\n",
    "print(header)\n",
    "for key in f1_dict:\n",
    "    values = f1_dict[key]\n",
    "    values += [np.mean(values), np.std(values)]\n",
    "    values = \",\".join([str(elem) for elem in values])\n",
    "    output_f1 += f\"{key},{values}\\n\"\n",
    "    print(key, \",\", values)\n",
    "print(\"\\n\\n\")\n",
    "print(\"c1 f1\")\n",
    "# print(header)\n",
    "# for key in class1_dict_acc:\n",
    "#     values = class1_dict_acc[key]\n",
    "#     values += [np.mean(values), np.std(values)]\n",
    "#     values = \",\".join([str(elem) for elem in values])\n",
    "#     class1_acc += f\"{key},{values}\\n\"\n",
    "#     print(key, \",\", values)  \n",
    "print(header)\n",
    "for key in class1_dict_f1:\n",
    "    values = class1_dict_f1[key]\n",
    "    values += [np.mean(values), np.std(values)]\n",
    "    values = \",\".join([str(elem) for elem in values])\n",
    "    class1_f1 += f\"{key},{values}\\n\"\n",
    "    print(key, \",\", values)\n",
    "print(\"\\n\\n\")\n",
    "print(\"c1 pr\")  \n",
    "print(header)\n",
    "for key in class1_dict_pr:\n",
    "    values = class1_dict_pr[key]\n",
    "    values += [np.mean(values), np.std(values)]\n",
    "    values = \",\".join([str(elem) for elem in values])\n",
    "    class1_pr += f\"{key},{values}\\n\"\n",
    "    print(key, \",\", values)\n",
    "print(\"\\n\\n\")\n",
    "print(\"c1 re\")\n",
    "print(header)\n",
    "for key in class1_dict_re:\n",
    "    values = class1_dict_re[key]\n",
    "    values += [np.mean(values), np.std(values)]\n",
    "    values = \",\".join([str(elem) for elem in values])\n",
    "    class1_re += f\"{key},{values}\\n\"\n",
    "    print(key, \",\", values)\n",
    "print(\"\\n\\n\")\n",
    "print(\"c0 f1\")\n",
    "\n",
    "# print(header)\n",
    "# for key in class0_dict_acc:\n",
    "#     values = class0_dict_acc[key]\n",
    "#     values += [np.mean(values), np.std(values)]\n",
    "#     values = \",\".join([str(elem) for elem in values])\n",
    "#     class0_acc += f\"{key},{values}\\n\"\n",
    "#     print(key, \",\", values)  \n",
    "print(header)\n",
    "for key in class0_dict_f1:\n",
    "    values = class0_dict_f1[key]\n",
    "    values += [np.mean(values), np.std(values)]\n",
    "    values = \",\".join([str(elem) for elem in values])\n",
    "    class0_f1 += f\"{key},{values}\\n\"\n",
    "    print(key, \",\", values) \n",
    "print(\"\\n\\n\")\n",
    "print(\"c0 pr\") \n",
    "print(header)\n",
    "for key in class0_dict_pr:\n",
    "    values = class0_dict_pr[key]\n",
    "    values += [np.mean(values), np.std(values)]\n",
    "    values = \",\".join([str(elem) for elem in values])\n",
    "    class0_pr += f\"{key},{values}\\n\"\n",
    "    print(key, \",\", values)\n",
    "print(\"\\n\\n\")\n",
    "print(\"c0 re\")\n",
    "print(header)\n",
    "for key in class0_dict_re:\n",
    "    values = class0_dict_re[key]\n",
    "    values += [np.mean(values), np.std(values)]\n",
    "    values = \",\".join([str(elem) for elem in values])\n",
    "    class0_re += f\"{key},{values}\\n\"\n",
    "    print(key, \",\", values) \n",
    "with open(\"acc_results_5.csv\", \"w\") as fptr:\n",
    "    fptr.write(output_acc)\n",
    "    fptr.close()\n",
    "with open(\"f1_results_5.csv\", \"w\") as fptr:\n",
    "    fptr.write(output_f1)\n",
    "    fptr.close()\n",
    "\n",
    "# with open(\"class1_acc_results.csv\", \"w\") as fptr:\n",
    "#     fptr.write(class1_acc)\n",
    "#     fptr.close()\n",
    "with open(\"class1_f1_results_5.csv\", \"w\") as fptr:\n",
    "    fptr.write(class1_f1)\n",
    "    fptr.close()\n",
    "with open(\"class1_pr_results_5.csv\", \"w\") as fptr:\n",
    "    fptr.write(class1_pr)\n",
    "    fptr.close()\n",
    "with open(\"class1_re_results_5.csv\", \"w\") as fptr:\n",
    "    fptr.write(class1_re)\n",
    "    fptr.close()\n",
    "\n",
    "# with open(\"class0_acc_results.csv\", \"w\") as fptr:\n",
    "#     fptr.write(class0_acc)\n",
    "#     fptr.close()\n",
    "with open(\"class0_f1_results_5.csv\", \"w\") as fptr:\n",
    "    fptr.write(class0_f1)\n",
    "    fptr.close()\n",
    "with open(\"class0_pr_results_5.csv\", \"w\") as fptr:\n",
    "    fptr.write(class0_pr)\n",
    "    fptr.close()\n",
    "with open(\"class0_re_results_5.csv\", \"w\") as fptr:\n",
    "    fptr.write(class0_re)\n",
    "    fptr.close()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "FineTuneLOOCV_2020_GOLD_2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
