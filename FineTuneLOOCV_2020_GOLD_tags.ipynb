{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "FineTuneLOOCV_2020_GOLD tags.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6fb0e6fcd4ca4d01a7d15443ef0b4637": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_091dc0a3037b474ab57d1736a4f9cdd6",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_ed5d8a594ee44f50966b54f502d74d51",
              "IPY_MODEL_8dbd4e90723f4e6f8dcade0fcad7b7f2"
            ]
          },
          "model_module_version": "1.5.0"
        },
        "091dc0a3037b474ab57d1736a4f9cdd6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          },
          "model_module_version": "1.2.0"
        },
        "ed5d8a594ee44f50966b54f502d74d51": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_9d3cf338e2c04c0285a6d63b71283ee3",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 231508,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 231508,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_618d5fd4edc54ff2866715eaec611d13"
          },
          "model_module_version": "1.5.0"
        },
        "8dbd4e90723f4e6f8dcade0fcad7b7f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_7c2492668b3a4a1d863c105ebefcde05",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 232k/232k [00:02&lt;00:00, 113kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b9820b33d99a44178c38f607afb8582c"
          },
          "model_module_version": "1.5.0"
        },
        "9d3cf338e2c04c0285a6d63b71283ee3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          },
          "model_module_version": "1.5.0"
        },
        "618d5fd4edc54ff2866715eaec611d13": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          },
          "model_module_version": "1.2.0"
        },
        "7c2492668b3a4a1d863c105ebefcde05": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          },
          "model_module_version": "1.5.0"
        },
        "b9820b33d99a44178c38f607afb8582c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          },
          "model_module_version": "1.2.0"
        },
        "b50a8f44e1804c41adeeaf3584ba1fdf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_3d118216fd95495cad7633bb6adc4033",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_749b977320784b66855efea55fff2f71",
              "IPY_MODEL_2f3ad32dc9244b9e989718e50fca8705"
            ]
          },
          "model_module_version": "1.5.0"
        },
        "3d118216fd95495cad7633bb6adc4033": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          },
          "model_module_version": "1.2.0"
        },
        "749b977320784b66855efea55fff2f71": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_6e235259e9fa4c48a3b078e84303ca80",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 28,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 28,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_54fa224ff3464ad397bfe7518fbbc94d"
          },
          "model_module_version": "1.5.0"
        },
        "2f3ad32dc9244b9e989718e50fca8705": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_356c105775794344bcd2079d8336f2e8",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 28.0/28.0 [00:00&lt;00:00, 44.5B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5a7fdaad8c194e279bafbb13b9ddb35e"
          },
          "model_module_version": "1.5.0"
        },
        "6e235259e9fa4c48a3b078e84303ca80": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          },
          "model_module_version": "1.5.0"
        },
        "54fa224ff3464ad397bfe7518fbbc94d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          },
          "model_module_version": "1.2.0"
        },
        "356c105775794344bcd2079d8336f2e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          },
          "model_module_version": "1.5.0"
        },
        "5a7fdaad8c194e279bafbb13b9ddb35e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          },
          "model_module_version": "1.2.0"
        },
        "300732dd250e4968918798260434b6e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_20965c8a79a04a539af1ca7ff802090c",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_050e272a837e49609fa535ba024f6b34",
              "IPY_MODEL_504c214365c541b88129eb44f097f017"
            ]
          },
          "model_module_version": "1.5.0"
        },
        "20965c8a79a04a539af1ca7ff802090c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          },
          "model_module_version": "1.2.0"
        },
        "050e272a837e49609fa535ba024f6b34": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_3375aaee07bd49be8d2c84a9d65d1cac",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 466062,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 466062,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_805a9e742d5d42ef987747e36731a4bd"
          },
          "model_module_version": "1.5.0"
        },
        "504c214365c541b88129eb44f097f017": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_9ca7de17a54f410c82b00c56ba2d99fe",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 466k/466k [00:00&lt;00:00, 2.77MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e3e6e6643baa419481906a4323a67fad"
          },
          "model_module_version": "1.5.0"
        },
        "3375aaee07bd49be8d2c84a9d65d1cac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          },
          "model_module_version": "1.5.0"
        },
        "805a9e742d5d42ef987747e36731a4bd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          },
          "model_module_version": "1.2.0"
        },
        "9ca7de17a54f410c82b00c56ba2d99fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          },
          "model_module_version": "1.5.0"
        },
        "e3e6e6643baa419481906a4323a67fad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          },
          "model_module_version": "1.2.0"
        },
        "df505c2b04164b02b71b4995cee56a9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_52df82aaca944f08a22207078e97b210",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_521f784301dd472090a3253fdfa3f17b",
              "IPY_MODEL_db976e4669bb442ab5f40b38fc659e13"
            ]
          },
          "model_module_version": "1.5.0"
        },
        "52df82aaca944f08a22207078e97b210": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          },
          "model_module_version": "1.2.0"
        },
        "521f784301dd472090a3253fdfa3f17b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_040a089770c34a2490035a873ada3e90",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 570,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 570,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_74c3ccc8187445d7b8807e8839ffcd5f"
          },
          "model_module_version": "1.5.0"
        },
        "db976e4669bb442ab5f40b38fc659e13": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_2250d81e41464e6a8af9b73794986f37",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 570/570 [00:00&lt;00:00, 1.25kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_054f3f4f4e284e2abb6752626c291489"
          },
          "model_module_version": "1.5.0"
        },
        "040a089770c34a2490035a873ada3e90": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          },
          "model_module_version": "1.5.0"
        },
        "74c3ccc8187445d7b8807e8839ffcd5f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          },
          "model_module_version": "1.2.0"
        },
        "2250d81e41464e6a8af9b73794986f37": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          },
          "model_module_version": "1.5.0"
        },
        "054f3f4f4e284e2abb6752626c291489": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          },
          "model_module_version": "1.2.0"
        },
        "d3ae222619b84f99ae7fa47a674685c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_10b8d797aacc4aed8b87181fd109c25c",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_922e1cb9ffd24490a0c268970c78cc5c",
              "IPY_MODEL_1573553abe4b43248df42b4c3e567414"
            ]
          },
          "model_module_version": "1.5.0"
        },
        "10b8d797aacc4aed8b87181fd109c25c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          },
          "model_module_version": "1.2.0"
        },
        "922e1cb9ffd24490a0c268970c78cc5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_09be425addd04dd2850ca0e49e68a002",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 440473133,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 440473133,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b5c111faba914a1fbce65a33ee0f3a2e"
          },
          "model_module_version": "1.5.0"
        },
        "1573553abe4b43248df42b4c3e567414": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_465c0373f1df44929a871fd0cc7dfbb3",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 440M/440M [00:09&lt;00:00, 48.2MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0dc59eec65844ca98b4ed2e3f2d414b5"
          },
          "model_module_version": "1.5.0"
        },
        "09be425addd04dd2850ca0e49e68a002": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          },
          "model_module_version": "1.5.0"
        },
        "b5c111faba914a1fbce65a33ee0f3a2e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          },
          "model_module_version": "1.2.0"
        },
        "465c0373f1df44929a871fd0cc7dfbb3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          },
          "model_module_version": "1.5.0"
        },
        "0dc59eec65844ca98b4ed2e3f2d414b5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          },
          "model_module_version": "1.2.0"
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AshwinDeshpande96/Measuring_the_Impact_of_Verbal_Disfluency_Tags_on_Automated_Dementia_Detection/blob/master/FineTuneLOOCV_2020_GOLD_tags.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V8HlwwyCPJSX",
        "outputId": "d3ffc7d8-a32d-4823-bcd4-f735f338a425"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d5/43/cfe4ee779bbd6a678ac6a97c5a5cdeb03c35f9eaebbb9720b036680f9a2d/transformers-4.6.1-py3-none-any.whl (2.2MB)\n",
            "\u001b[K     |████████████████████████████████| 2.3MB 5.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Collecting huggingface-hub==0.0.8\n",
            "  Downloading https://files.pythonhosted.org/packages/a1/88/7b1e45720ecf59c6c6737ff332f41c955963090a18e72acbcbeac6b25e86/huggingface_hub-0.0.8-py3-none-any.whl\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 46.7MB/s \n",
            "\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/e2/df3543e8ffdab68f5acc73f613de9c2b155ac47f162e725dcac87c521c11/tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 49.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.0.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Installing collected packages: huggingface-hub, sacremoses, tokenizers, transformers\n",
            "Successfully installed huggingface-hub-0.0.8 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.6.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5eq2FbbxUkH"
      },
      "source": [
        "# models = ['Bert', 'Roberta', 'DistilBERT']\n",
        "# _ = [print(f\"[{i + 1}] {m}\") for i, m in enumerate(models)]\n",
        "# model_num = int(input(\"Choose model: \")) - 1\n",
        "# if model_num not in range(len(models)):\n",
        "#     raise Exception(\"Incorrect model chosen.\")\n",
        "\n",
        "# model_name = models[model_num]\n",
        "#####################################################################\n",
        "import random\n",
        "import re\n",
        "import csv\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "from torch.utils.data import TensorDataset, random_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "from google.colab import drive\n",
        "import datetime\n",
        "from transformers import AdamW\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from sklearn.model_selection import LeaveOneOut\n",
        "import time\n",
        "import itertools\n",
        "import matplotlib.pyplot as plt\n",
        "% matplotlib inline\n",
        "from sklearn.model_selection import KFold\n",
        "import pickle\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "# drive.mount('/content/drive')\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "\n",
        "#################### Global Variables ####################\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "UNUSED_TOKEN = {1: '[unused0]',\n",
        "                2: '[unused1]',\n",
        "                3: '[unused2]'}\n",
        "to_categorical = {'Control': 0,\n",
        "                  'Dementia': 1}\n",
        "\n",
        "################### Train and Test Sets ####################\n",
        "\n",
        "# train_text, temp_text, train_labels, temp_labels = train_test_split(df['transcript'], df['dx'],\n",
        "#                                                                     random_state=seed_val,\n",
        "#                                                                     test_size=0.3,\n",
        "#                                                                     stratify=df['dx'])\n",
        "\n",
        "\n",
        "#################### BERT features ####################\n",
        "\n",
        "\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
        "\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "\n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
        "\n",
        "def get_bert_tokenizer():\n",
        "    from transformers import BertTokenizer\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_basic_tokenize=False)\n",
        "    tokenizer.add_special_tokens({ \"additional_special_tokens\": [ \"[unused0]\" ,\"[unused1]\" ,\"[unused2]\" ,\"[unused3]\"] })\n",
        "    return tokenizer\n",
        "\n",
        "def get_roberta_tokenizer():\n",
        "    from transformers import RobertaTokenizer\n",
        "    tokenizer = RobertaTokenizer.from_pretrained('roberta-base', do_basic_tokenize=False)\n",
        "    tokenizer.add_special_tokens({ \"additional_special_tokens\": [ \"[unused0]\" ] })\n",
        "    return tokenizer\n",
        "\n",
        "def get_distilbert_tokenizer():\n",
        "    from transformers import DistilBertTokenizer\n",
        "    tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased', do_basic_tokenize=False)\n",
        "    tokenizer.add_special_tokens({ \"additional_special_tokens\": [ \"[unused0]\" ] })\n",
        "    return tokenizer\n",
        "\n",
        "def encode_sentence(transcript, tokenizer):\n",
        "    transcript = re.sub(r\"\\.\\s\", \" [SEP]\", transcript)\n",
        "    transcript = transcript.replace(r\"<\", \" [unused0]\")\n",
        "    transcript = transcript.replace(r\">\", \" [unused1]\")\n",
        "    transcript = transcript.replace(r\"[/]\", \" [unused2]\")\n",
        "    transcript = transcript.replace(r\"[//]\", \" [unused3]\")\n",
        "    tokens = list(tokenizer.tokenize(transcript))\n",
        "    tokens = ['[CLS]'] + tokens + ['[SEP]']\n",
        "    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "    return token_ids\n",
        "\n",
        "\n",
        "def add_padding(input_word_ids, max_seq_len):\n",
        "    input_type = []\n",
        "    for idx, embedding in enumerate(input_word_ids):\n",
        "        embedding_len = len(embedding)\n",
        "        e_input_type = np.ones(embedding_len, dtype=np.int64).tolist()\n",
        "        if embedding_len < max_seq_len:\n",
        "            zeros = np.zeros(max_seq_len - embedding_len, dtype=np.int64).tolist()\n",
        "            e_input_type += zeros\n",
        "            embedding += zeros\n",
        "        elif embedding_len > max_seq_len:\n",
        "            embedding = embedding[:max_seq_len - 1] + [102]\n",
        "            e_input_type = e_input_type[:max_seq_len]\n",
        "        input_type.append(torch.tensor([e_input_type]))\n",
        "        input_word_ids[idx] = torch.tensor([embedding])\n",
        "    return {'input_ids': input_word_ids, 'attention_mask': input_type}\n",
        "\n",
        "\n",
        "def bert_encode(transcripts, tokenizer, max_seq_len):\n",
        "    input_word_ids = [encode_sentence(s, tokenizer)\n",
        "                      for s in transcripts]\n",
        "    input_word_ids = add_padding(input_word_ids, max_seq_len)\n",
        "    return input_word_ids\n",
        "############################# Models ##################################\n",
        "def get_bert_model():\n",
        "    from transformers import BertForSequenceClassification\n",
        "    model = BertForSequenceClassification.from_pretrained(\n",
        "        \"bert-base-uncased\",  # Use the 12-layer BERT model, with an uncased vocab.\n",
        "        num_labels=2,  # The number of output labels--2 for binary classification.\n",
        "        # You can increase this for multi-class tasks.\n",
        "        output_attentions=False,  # Whether the model returns attentions weights.\n",
        "        output_hidden_states=False,  # Whether the model returns all hidden-states.\n",
        "    )\n",
        "    model.cuda()\n",
        "    return model\n",
        "\n",
        "\n",
        "def get_roberta_model():\n",
        "    from transformers import RobertaForSequenceClassification\n",
        "    model = RobertaForSequenceClassification.from_pretrained(\n",
        "        \"roberta-base\",  # Use the 12-layer BERT model, with an uncased vocab.\n",
        "        num_labels=2,  # The number of output labels--2 for binary classification.\n",
        "        # You can increase this for multi-class tasks.\n",
        "        output_attentions=False,  # Whether the model returns attentions weights.\n",
        "        output_hidden_states=False,  # Whether the model returns all hidden-states.\n",
        "    )\n",
        "    model.cuda()\n",
        "    return model\n",
        "\n",
        "def get_distilbert_model():\n",
        "    from transformers import DistilBertForSequenceClassification\n",
        "    model = DistilBertForSequenceClassification.from_pretrained(\n",
        "        \"distilbert-base-uncased\",  # Use the 12-layer BERT model, with an uncased vocab.\n",
        "        num_labels=2,  # The number of output labels--2 for binary classification.\n",
        "        # You can increase this for multi-class tasks.\n",
        "        output_attentions=False,  # Whether the model returns attentions weights.\n",
        "        output_hidden_states=False,  # Whether the model returns all hidden-states.\n",
        "    )\n",
        "    model.cuda()\n",
        "    return model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sePSgdTMuS1w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53ca4b91-8fbc-46dd-f269-0b770eb32459"
      },
      "source": [
        "  from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/My\\ Drive/Research/ADReSSo\n",
        "# %cd /content/drive/My\\ Drive/Admission/UIC/Research/Python/Dementia-Detection/Disfluency-Detection/english-fisher-annotations\n",
        "!pwd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/My Drive/Research/ADReSSo\n",
            "/content/drive/My Drive/Research/ADReSSo\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Xvqyf2UFeKu"
      },
      "source": [
        "def reps():\n",
        "  df = pd.read_pickle('data2020fisher_reps.pickle')\n",
        "  X1 = df.transcript_without_tags.to_numpy()\n",
        "  X2 = df.transcript_with_stopword_rep.to_numpy()\n",
        "  X3 = df.transcript_with_non_stopword_rep.to_numpy()\n",
        "  X4 = df.transcript_with_no_rep.to_numpy()\n",
        "  X5 = df.transcript_with_no_filled_pauses.to_numpy()\n",
        "  experiments = ['transcript_without_tags',\n",
        "                  'transcript_with_stopword_rep',\n",
        "                  'transcript_with_non_stopword_rep',\n",
        "                  'transcript_with_no_rep',\n",
        "                  'transcript_with_no_filled_pauses']\n",
        "  Xs = [X1, X2, X3, X4, X5]\n",
        "  df.dx = df.dx.apply(lambda x: to_categorical[x])\n",
        "  y = df.dx.to_numpy()\n",
        "  return df, experiments, Xs, y\n",
        "\n",
        "def all_exp():\n",
        "  df = pd.read_pickle('data2020fisher_all.pickle')\n",
        "  experiments = ['transcript_without_tags',\n",
        "                 'transcript_without_underscore',\n",
        "                 'transcript_without_repetition',\n",
        "                 'transcript_without_retracing',\n",
        "                 'transcript_without_disfluency',\n",
        "                ]\n",
        "  X1 = df.transcript_without_tags.to_numpy()\n",
        "  X2 = df.transcript_without_underscore.to_numpy()\n",
        "  X3 = df.transcript_without_repetition.to_numpy()\n",
        "  X4 = df.transcript_without_retracing.to_numpy()\n",
        "  X5 = df.transcript_without_disfluency.to_numpy()\n",
        "  Xs = [X1, X2, X3, X4, X5]\n",
        "  df.dx = df.dx.apply(lambda x: to_categorical[x])\n",
        "  y = df.dx.to_numpy()\n",
        "  return df, experiments, Xs, y\n",
        "def asr_data():\n",
        "    df = pd.read_pickle('new_data.pickle').dropna()\n",
        "    df = df[df.both_eremoved != \"placeholder\"]\n",
        "    print(df.columns.tolist())\n",
        "    experiments = [\"all_errors\", \"e_tagged\", \"rep_eremoved\", \"ret_eremoved\", \"both_eremoved\"]\n",
        "    df.dx = df.dx.apply(lambda x: to_categorical[x])\n",
        "    # X1 = df.all_errors.to_numpy()\n",
        "    X2 = df.e_tagged.to_numpy()\n",
        "    X3 = df.rep_eremoved.to_numpy()\n",
        "    X4 = df.ret_eremoved.to_numpy()\n",
        "    X5 = df.both_eremoved.to_numpy()\n",
        "    # Xs = [X1, X2, X3, X4, X5]\n",
        "    Xs = [X2, X3, X4, X5]\n",
        "    y = df.dx.to_numpy()\n",
        "    return df, experiments, Xs, y\n",
        "\n",
        "def only_asr():\n",
        "    df = pd.read_pickle('new_data.pickle').dropna()\n",
        "    df = df[df.both_eremoved != \"placeholder\"]\n",
        "    experiments = ['all_errors']\n",
        "    df.dx = df.dx.apply(lambda x: to_categorical[x])\n",
        "    X1 = df.all_errors.to_numpy()\n",
        "    Xs = [X1]\n",
        "    y = df.dx.to_numpy()\n",
        "    return df, experiments, Xs, y\n",
        "\n",
        "def gold_with_tags():\n",
        "    df = pd.read_pickle('data2020gold_with_tags.pickle')\n",
        "    experiments = ['transcript_with_tags2',\n",
        "                    'transcript_without_repetition',\n",
        "                    'transcript_without_retracing',\n",
        "                   'transcript_without_either'\n",
        "                   ]\n",
        "    df.dx = df.dx.apply(lambda x: to_categorical[x])\n",
        "    X1 = df.transcript_with_tags2.to_numpy()\n",
        "    X2 = df.transcript_without_repetition.to_numpy()\n",
        "    X3 = df.transcript_without_retracing.to_numpy()\n",
        "    X4 = df.transcript_without_either.to_numpy()\n",
        "    Xs = [X1, X2, X3, X4]\n",
        "    y = df.dx.to_numpy()\n",
        "    return df, experiments, Xs, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nMLC4h8xCpLj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "outputId": "ebfc1cdb-b842-4899-ac6e-05aac28bdd07"
      },
      "source": [
        "df, experiments, Xs, y = gold_with_tags()\n",
        "print(df.shape)\n",
        "df.sample(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(108, 7)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>speaker</th>\n",
              "      <th>transcript_with_tags2</th>\n",
              "      <th>transcript_without_repetition</th>\n",
              "      <th>transcript_without_retracing</th>\n",
              "      <th>transcript_without_either</th>\n",
              "      <th>dx</th>\n",
              "      <th>mmse</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>61</th>\n",
              "      <td>S087</td>\n",
              "      <td>hm &lt;a lady&gt; [/] a lady and her children. child...</td>\n",
              "      <td>hm a lady and her children. children. the lady...</td>\n",
              "      <td>hm a lady a lady and her children. children. t...</td>\n",
              "      <td>hm a lady and her children. children. the lady...</td>\n",
              "      <td>1</td>\n",
              "      <td>7.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>S143</td>\n",
              "      <td>well little boy throat reaching out for the co...</td>\n",
              "      <td>well little boy throat reaching out for the co...</td>\n",
              "      <td>well little boy throat reaching out for the co...</td>\n",
              "      <td>well little boy throat reaching out for the co...</td>\n",
              "      <td>1</td>\n",
              "      <td>18.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>S064</td>\n",
              "      <td>the kids are swiping some cookies and [/] uh a...</td>\n",
              "      <td>the kids are swiping some cookies uh and a sto...</td>\n",
              "      <td>the kids are swiping some cookies and uh and a...</td>\n",
              "      <td>the kids are swiping some cookies uh and a sto...</td>\n",
              "      <td>0</td>\n",
              "      <td>29.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>92</th>\n",
              "      <td>S137</td>\n",
              "      <td>hm the woman of the house is drying the dishes...</td>\n",
              "      <td>hm the woman of the house is drying the dishes...</td>\n",
              "      <td>hm the woman of the house is drying the dishes...</td>\n",
              "      <td>hm the woman of the house is drying the dishes...</td>\n",
              "      <td>1</td>\n",
              "      <td>23.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66</th>\n",
              "      <td>S094</td>\n",
              "      <td>oh yes. a little girl a and the little boy is ...</td>\n",
              "      <td>oh yes. a little girl a and the little boy is ...</td>\n",
              "      <td>oh yes. a little girl a and the little boy is ...</td>\n",
              "      <td>oh yes. a little girl a and the little boy is ...</td>\n",
              "      <td>1</td>\n",
              "      <td>17.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>S056</td>\n",
              "      <td>uh it's uh a kitchen scene. and the mother is ...</td>\n",
              "      <td>uh it's uh a kitchen scene. and the mother is ...</td>\n",
              "      <td>uh it's uh a kitchen scene. and the mother is ...</td>\n",
              "      <td>uh it's uh a kitchen scene. and the mother is ...</td>\n",
              "      <td>0</td>\n",
              "      <td>30.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>S063</td>\n",
              "      <td>okay. the boy is standing up trying to get the...</td>\n",
              "      <td>okay. the boy is standing up trying to get the...</td>\n",
              "      <td>okay. the boy is standing up trying to get the...</td>\n",
              "      <td>okay. the boy is standing up trying to get the...</td>\n",
              "      <td>0</td>\n",
              "      <td>29.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>S024</td>\n",
              "      <td>alright. um the mother is standing at the kitc...</td>\n",
              "      <td>alright. um the mother is standing at the kitc...</td>\n",
              "      <td>alright. um the mother is standing at the kitc...</td>\n",
              "      <td>alright. um the mother is standing at the kitc...</td>\n",
              "      <td>0</td>\n",
              "      <td>30.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69</th>\n",
              "      <td>S097</td>\n",
              "      <td>an average home that looks very much like ours...</td>\n",
              "      <td>an average home that looks very much like ours...</td>\n",
              "      <td>an average home that looks very much like ours...</td>\n",
              "      <td>an average home that looks very much like ours...</td>\n",
              "      <td>1</td>\n",
              "      <td>15.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>S035</td>\n",
              "      <td>hm. touching lip. raising arm. is that what yo...</td>\n",
              "      <td>hm. touching lip. raising arm. is that what yo...</td>\n",
              "      <td>hm. touching lip. raising arm. is that what yo...</td>\n",
              "      <td>hm. touching lip. raising arm. is that what yo...</td>\n",
              "      <td>0</td>\n",
              "      <td>30.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   speaker                              transcript_with_tags2  ... dx  mmse\n",
              "61    S087  hm <a lady> [/] a lady and her children. child...  ...  1   7.0\n",
              "98    S143  well little boy throat reaching out for the co...  ...  1  18.0\n",
              "45    S064  the kids are swiping some cookies and [/] uh a...  ...  0  29.0\n",
              "92    S137  hm the woman of the house is drying the dishes...  ...  1  23.0\n",
              "66    S094  oh yes. a little girl a and the little boy is ...  ...  1  17.0\n",
              "39    S056  uh it's uh a kitchen scene. and the mother is ...  ...  0  30.0\n",
              "44    S063  okay. the boy is standing up trying to get the...  ...  0  29.0\n",
              "18    S024  alright. um the mother is standing at the kitc...  ...  0  30.0\n",
              "69    S097  an average home that looks very much like ours...  ...  1  15.0\n",
              "27    S035  hm. touching lip. raising arm. is that what yo...  ...  0  30.0\n",
              "\n",
              "[10 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PbF4kWxnIbU6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "6fb0e6fcd4ca4d01a7d15443ef0b4637",
            "091dc0a3037b474ab57d1736a4f9cdd6",
            "ed5d8a594ee44f50966b54f502d74d51",
            "8dbd4e90723f4e6f8dcade0fcad7b7f2",
            "9d3cf338e2c04c0285a6d63b71283ee3",
            "618d5fd4edc54ff2866715eaec611d13",
            "7c2492668b3a4a1d863c105ebefcde05",
            "b9820b33d99a44178c38f607afb8582c",
            "b50a8f44e1804c41adeeaf3584ba1fdf",
            "3d118216fd95495cad7633bb6adc4033",
            "749b977320784b66855efea55fff2f71",
            "2f3ad32dc9244b9e989718e50fca8705",
            "6e235259e9fa4c48a3b078e84303ca80",
            "54fa224ff3464ad397bfe7518fbbc94d",
            "356c105775794344bcd2079d8336f2e8",
            "5a7fdaad8c194e279bafbb13b9ddb35e",
            "300732dd250e4968918798260434b6e0",
            "20965c8a79a04a539af1ca7ff802090c",
            "050e272a837e49609fa535ba024f6b34",
            "504c214365c541b88129eb44f097f017",
            "3375aaee07bd49be8d2c84a9d65d1cac",
            "805a9e742d5d42ef987747e36731a4bd",
            "9ca7de17a54f410c82b00c56ba2d99fe",
            "e3e6e6643baa419481906a4323a67fad",
            "df505c2b04164b02b71b4995cee56a9b",
            "52df82aaca944f08a22207078e97b210",
            "521f784301dd472090a3253fdfa3f17b",
            "db976e4669bb442ab5f40b38fc659e13",
            "040a089770c34a2490035a873ada3e90",
            "74c3ccc8187445d7b8807e8839ffcd5f",
            "2250d81e41464e6a8af9b73794986f37",
            "054f3f4f4e284e2abb6752626c291489",
            "d3ae222619b84f99ae7fa47a674685c8",
            "10b8d797aacc4aed8b87181fd109c25c",
            "922e1cb9ffd24490a0c268970c78cc5c",
            "1573553abe4b43248df42b4c3e567414",
            "09be425addd04dd2850ca0e49e68a002",
            "b5c111faba914a1fbce65a33ee0f3a2e",
            "465c0373f1df44929a871fd0cc7dfbb3",
            "0dc59eec65844ca98b4ed2e3f2d414b5"
          ]
        },
        "outputId": "007122b6-98cd-4e21-9fbc-f69b054e01d3"
      },
      "source": [
        "model_name = \"bert\"\n",
        "lr = 4e-5\n",
        "max_seq_len = 256\n",
        "epochs = 8\n",
        "# epochs = 1\n",
        "batch_size_tr = 16\n",
        "batch_size_ts = 32\n",
        "from collections import OrderedDict\n",
        "# experiments = ['transcript_without_tags', 'transcript_without_retracing','transcript_without_repetition', 'transcript_without_disfluency']\n",
        "accuracy_dict = OrderedDict(zip(experiments, [[] for _ in range(len(experiments))]))\n",
        "f1_dict = OrderedDict(zip(experiments, [[] for _ in range(len(experiments))]))\n",
        "# class1_dict_acc = OrderedDict(zip(experiments, [[] for _ in range(len(experiments))]))\n",
        "class1_dict_f1 = OrderedDict(zip(experiments, [[] for _ in range(len(experiments))]))\n",
        "class1_dict_pr = OrderedDict(zip(experiments, [[] for _ in range(len(experiments))]))\n",
        "class1_dict_re = OrderedDict(zip(experiments, [[] for _ in range(len(experiments))]))\n",
        "\n",
        "# class0_dict_acc = OrderedDict(zip(experiments, [[] for _ in range(len(experiments))]))\n",
        "class0_dict_f1 = OrderedDict(zip(experiments, [[] for _ in range(len(experiments))]))\n",
        "class0_dict_pr = OrderedDict(zip(experiments, [[] for _ in range(len(experiments))]))\n",
        "class0_dict_re = OrderedDict(zip(experiments, [[] for _ in range(len(experiments))]))\n",
        "loo = LeaveOneOut()\n",
        "\n",
        "\n",
        "# print(f\"SEED: {seed_val}\\nMODEL: {model_name}\\nBATCH SIZE TR: {batch_size_tr}\\nBATCH SIZE TS: {batch_size_ts}\\nLEARNING RATE: {lr}\\nEMBEDDING LEN: {max_seq_len}\\nEPOCH: {epochs}\")\n",
        "# for seed_val in [0, 42, 23, 17, 19, 111]:\n",
        "for seed_val in [0, 23, 111]:\n",
        "    for name, X in zip(experiments, Xs):\n",
        "        print(\"#\"*10, name)\n",
        "        random.seed(seed_val)\n",
        "        np.random.seed(seed_val)\n",
        "        torch.manual_seed(seed_val)\n",
        "        torch.cuda.manual_seed_all(seed_val)\n",
        "        kf = KFold(n_splits=5, random_state=seed_val, shuffle=True)\n",
        "        total_preds = []\n",
        "        total_target = []\n",
        "        f1_set = []\n",
        "        acc_set = []\n",
        "        fold=1\n",
        "        for train_index, test_index in kf.split(X):\n",
        "            print('fold %d'%(fold))\n",
        "            random.seed(seed_val)\n",
        "            np.random.seed(seed_val)\n",
        "            torch.manual_seed(seed_val)\n",
        "            torch.cuda.manual_seed_all(seed_val)\n",
        "            tokenizer = locals()[f\"get_{model_name.lower()}_tokenizer\"]()\n",
        "            model = locals()[f\"get_{model_name.lower()}_model\"]()\n",
        "            train_text, temp_text = X[train_index], X[test_index]\n",
        "            train_labels, temp_labels = y[train_index], y[test_index]\n",
        "            fold+=1\n",
        "            \n",
        "            tokens_train = bert_encode(train_text.tolist(), tokenizer, max_seq_len)\n",
        "            tokens_test = bert_encode(temp_text.tolist(), tokenizer, max_seq_len)\n",
        "\n",
        "            #################### Torch Dataset ####################\n",
        "\n",
        "            input_ids = torch.cat(tokens_train['input_ids'], dim=0)\n",
        "            attention_masks = torch.cat(tokens_train['attention_mask'], dim=0)\n",
        "            labels = torch.tensor(train_labels.tolist())\n",
        "\n",
        "            input_ids_test = torch.cat(tokens_test['input_ids'], dim=0)\n",
        "            attention_masks_test = torch.cat(tokens_test['attention_mask'], dim=0)\n",
        "            labels_test = torch.tensor(temp_labels.tolist())\n",
        "\n",
        "\n",
        "            ####################\n",
        "            dataset = TensorDataset(input_ids, attention_masks, labels)\n",
        "            train_size = int(0.9 * len(dataset))\n",
        "            val_size = len(dataset) - train_size\n",
        "            train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "            ####################\n",
        "            train_dataloader = DataLoader(\n",
        "                train_dataset,  # The training samples.\n",
        "                sampler=RandomSampler(train_dataset),  # Select batches randomly\n",
        "                batch_size=batch_size_tr  # Trains with this batch size.\n",
        "            )\n",
        "\n",
        "            validation_dataloader = DataLoader(\n",
        "                val_dataset,  # The validation samples.\n",
        "                sampler=SequentialSampler(val_dataset),  # Pull out batches sequentially.\n",
        "                batch_size=batch_size_tr  # Evaluate with this batch size.\n",
        "            )\n",
        "            ###############################################################################\n",
        "\n",
        "            total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "            #################### Training stuff ####################\n",
        "            optimizer = AdamW(model.parameters(),\n",
        "                            lr=lr,  # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                            eps=1e-8  # args.adam_epsilon  - default is 1e-8.\n",
        "                            )\n",
        "\n",
        "            scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                                        num_warmup_steps=0,  # Default value in run_glue.py\n",
        "                                                        num_training_steps=total_steps)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            training_stats = []\n",
        "\n",
        "            # Measure the total training time for the whole run.\n",
        "            total_t0 = time.time()\n",
        "\n",
        "            # For each epoch...\n",
        "            for epoch_i in range(0, epochs):\n",
        "\n",
        "                # ========================================\n",
        "                #               Training\n",
        "                # ========================================\n",
        "\n",
        "                # Perform one full pass over the training set.\n",
        "\n",
        "                # print(\"\")\n",
        "                # print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "                # print('Training...')\n",
        "\n",
        "                # Measure how long the training epoch takes.\n",
        "                t0 = time.time()\n",
        "\n",
        "                # Reset the total loss for this epoch.\n",
        "                total_train_loss = 0\n",
        "\n",
        "                # Put the model into training mode. Don't be mislead--the call to\n",
        "                # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "                # `dropout` and `batchnorm` layers behave differently during training\n",
        "                # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "                model.train()\n",
        "\n",
        "                # For each batch of training data...\n",
        "                for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "                    # Progress update every 40 batches.\n",
        "                    if step % 40 == 0 and not step == 0:\n",
        "                        # Calculate elapsed time in minutes.\n",
        "                        elapsed = format_time(time.time() - t0)\n",
        "\n",
        "                        # Report progress.\n",
        "                        # print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "                    # Unpack this training batch from our dataloader.\n",
        "                    #\n",
        "                    # As we unpack the batch, we'll also copy each tensor to the GPU using the\n",
        "                    # `to` method.\n",
        "                    #\n",
        "                    # `batch` contains three pytorch tensors:\n",
        "                    #   [0]: input ids\n",
        "                    #   [1]: attention masks\n",
        "                    #   [2]: labels\n",
        "                    b_input_ids = batch[0].to(device)\n",
        "                    b_input_mask = batch[1].to(device)\n",
        "                    b_labels = batch[2].to(device)\n",
        "\n",
        "                    # Always clear any previously calculated gradients before performing a\n",
        "                    # backward pass. PyTorch doesn't do this automatically because\n",
        "                    # accumulating the gradients is \"convenient while training RNNs\".\n",
        "                    # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "                    model.zero_grad()\n",
        "\n",
        "                    # Perform a forward pass (evaluate the model on this training batch).\n",
        "                    # In PyTorch, calling `model` will in turn call the model's `forward`\n",
        "                    # function and pass down the arguments. The `forward` function is\n",
        "                    # documented here:\n",
        "                    # https://huggingface.co/transformers/model_doc/bert.html#bertforsequenceclassification\n",
        "                    # The results are returned in a results object, documented here:\n",
        "                    # https://huggingface.co/transformers/main_classes/output.html#transformers.modeling_outputs.SequenceClassifierOutput\n",
        "                    # Specifically, we'll get the loss (because we provided labels) and the\n",
        "                    # \"logits\"--the model outputs prior to activation.\n",
        "                    try:\n",
        "                        result = model(b_input_ids,\n",
        "                                    token_type_ids=None,\n",
        "                                    attention_mask=b_input_mask,\n",
        "                                    labels=b_labels,\n",
        "                                    return_dict=True)\n",
        "                    except:\n",
        "                        result = model(b_input_ids,\n",
        "                                        #    token_type_ids=None,\n",
        "                                        attention_mask=b_input_mask,\n",
        "                                        labels=b_labels,\n",
        "                                        return_dict=True)                    \n",
        "\n",
        "                    loss = result.loss\n",
        "                    logits = result.logits\n",
        "\n",
        "                    # Accumulate the training loss over all of the batches so that we can\n",
        "                    # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "                    # single value; the `.item()` function just returns the Python value\n",
        "                    # from the tensor.\n",
        "                    total_train_loss += loss.item()\n",
        "\n",
        "                    # Perform a backward pass to calculate the gradients.\n",
        "                    loss.backward()\n",
        "\n",
        "                    # Clip the norm of the gradients to 1.0.\n",
        "                    # This is to help prevent the \"exploding gradients\" problem.\n",
        "                    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "                    # Update parameters and take a step using the computed gradient.\n",
        "                    # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "                    # modified based on their gradients, the learning rate, etc.\n",
        "                    optimizer.step()\n",
        "\n",
        "                    # Update the learning rate.\n",
        "                    scheduler.step()\n",
        "\n",
        "                # Calculate the average loss over all of the batches.\n",
        "                avg_train_loss = total_train_loss / len(train_dataloader)\n",
        "\n",
        "                # Measure how long this epoch took.\n",
        "                training_time = format_time(time.time() - t0)\n",
        "\n",
        "                # print(\"\")\n",
        "                # print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "                # print(\"  Training epcoh took: {:}\".format(training_time))\n",
        "\n",
        "                # ========================================\n",
        "                #               Validation\n",
        "                # ========================================\n",
        "                # After the completion of each training epoch, measure our performance on\n",
        "                # our validation set.\n",
        "\n",
        "                # print(\"\")\n",
        "                # print(\"Running Validation...\")\n",
        "\n",
        "                t0 = time.time()\n",
        "\n",
        "                # Put the model in evaluation mode--the dropout layers behave differently\n",
        "                # during evaluation.\n",
        "                model.eval()\n",
        "\n",
        "                # Tracking variables\n",
        "                total_eval_accuracy = 0\n",
        "                total_eval_loss = 0\n",
        "                nb_eval_steps = 0\n",
        "\n",
        "                # Evaluate data for one epoch\n",
        "                for batch in validation_dataloader:\n",
        "                    # Unpack this training batch from our dataloader.\n",
        "                    #\n",
        "                    # As we unpack the batch, we'll also copy each tensor to the GPU using\n",
        "                    # the `to` method.\n",
        "                    #\n",
        "                    # `batch` contains three pytorch tensors:\n",
        "                    #   [0]: input ids\n",
        "                    #   [1]: attention masks\n",
        "                    #   [2]: labels\n",
        "                    b_input_ids = batch[0].to(device)\n",
        "                    b_input_mask = batch[1].to(device)\n",
        "                    b_labels = batch[2].to(device)\n",
        "\n",
        "                    # Tell pytorch not to bother with constructing the compute graph during\n",
        "                    # the forward pass, since this is only needed for backprop (training).\n",
        "                    with torch.no_grad():\n",
        "                        # Forward pass, calculate logit predictions.\n",
        "                        # token_type_ids is the same as the \"segment ids\", which\n",
        "                        # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "                        result = model(b_input_ids,\n",
        "                                    #    token_type_ids=None,\n",
        "                                    attention_mask=b_input_mask,\n",
        "                                    labels=b_labels,\n",
        "                                    return_dict=True)\n",
        "\n",
        "                    # Get the loss and \"logits\" output by the model. The \"logits\" are the\n",
        "                    # output values prior to applying an activation function like the\n",
        "                    # softmax.\n",
        "                    loss = result.loss\n",
        "                    logits = result.logits\n",
        "\n",
        "                    # Accumulate the validation loss.\n",
        "                    total_eval_loss += loss.item()\n",
        "\n",
        "                    # Move logits and labels to CPU\n",
        "                    logits = logits.detach().cpu().numpy()\n",
        "                    label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "                    # Calculate the accuracy for this batch of test sentences, and\n",
        "                    # accumulate it over all batches.\n",
        "                    total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
        "\n",
        "                # Report the final accuracy for this validation run.\n",
        "                avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
        "                # print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
        "\n",
        "                # Calculate the average loss over all of the batches.\n",
        "                avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
        "\n",
        "                # Measure how long the validation run took.\n",
        "                validation_time = format_time(time.time() - t0)\n",
        "\n",
        "                # print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
        "                # print(\"  Validation took: {:}\".format(validation_time))\n",
        "\n",
        "                # Record all statistics from this epoch.\n",
        "                training_stats.append(\n",
        "                    {\n",
        "                        'epoch': epoch_i + 1,\n",
        "                        'Training Loss': avg_train_loss,\n",
        "                        'Valid. Loss': avg_val_loss,\n",
        "                        'Valid. Accur.': avg_val_accuracy,\n",
        "                        'Training Time': training_time,\n",
        "                        'Validation Time': validation_time\n",
        "                    }\n",
        "                )\n",
        "\n",
        "            # print(\"\")\n",
        "            # print(\"Training complete!\")\n",
        "\n",
        "            # print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time() - total_t0)))\n",
        "\n",
        "            # Create the DataLoader.\n",
        "            prediction_data = TensorDataset(input_ids_test, attention_masks_test, labels_test)\n",
        "            prediction_sampler = SequentialSampler(prediction_data)\n",
        "            prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size_ts)\n",
        "            ################################################################################\n",
        "            # Prediction on test set\n",
        "\n",
        "            # print('Predicting labels for {:,} test sentences...'.format(len(input_ids)))\n",
        "\n",
        "            # Put model in evaluation mode\n",
        "            model.eval()\n",
        "\n",
        "            # Tracking variables \n",
        "            predictions , true_labels = [], []\n",
        "\n",
        "            # Predict \n",
        "            for batch in prediction_dataloader:\n",
        "                # Add batch to GPU\n",
        "                batch = tuple(t.to(device) for t in batch)\n",
        "            \n",
        "                # Unpack the inputs from our dataloader\n",
        "                b_input_ids, b_input_mask, b_labels = batch\n",
        "                \n",
        "                # Telling the model not to compute or store gradients, saving memory and \n",
        "                # speeding up prediction\n",
        "                with torch.no_grad():\n",
        "\n",
        "                    # Forward pass, calculate logit predictions.\n",
        "                    try:\n",
        "                        result = model(b_input_ids, \n",
        "                                        token_type_ids=None, \n",
        "                                        attention_mask=b_input_mask,\n",
        "                                        return_dict=True)\n",
        "                    except:\n",
        "                        result = model(b_input_ids, \n",
        "                                        #  token_type_ids=None, \n",
        "                                        attention_mask=b_input_mask,\n",
        "                                        return_dict=True)\n",
        "\n",
        "                logits = result.logits\n",
        "\n",
        "                # Move logits and labels to CPU\n",
        "                logits = logits.detach().cpu().numpy()\n",
        "                label_ids = b_labels.to('cpu').numpy()\n",
        "                \n",
        "                # Store predictions and true labels\n",
        "                predictions.append(logits)\n",
        "                true_labels.append(label_ids)\n",
        "\n",
        "            # print('    DONE.')\n",
        "            ###############################################################################\n",
        "            \n",
        "            \n",
        "\n",
        "            # Evaluate each test batch using Matthew's correlation coefficient\n",
        "            # print('Calculating Acc and F1 Corr. Coef. for each batch...')\n",
        "\n",
        "            # For each input batch...\n",
        "            for i in range(len(true_labels)):\n",
        "                # The predictions for this batch are a 2-column ndarray (one column for \"0\" \n",
        "                # and one column for \"1\"). Pick the label with the highest value and turn this\n",
        "                # in to a list of 0s and 1s.\n",
        "                pred_labels_i = np.argmax(predictions[i], axis=1).flatten()\n",
        "                total_target += true_labels[i].tolist()\n",
        "                total_preds += pred_labels_i.tolist()\n",
        "                \n",
        "                # Calculate and store the acc for this batch.\n",
        "                acc = accuracy_score(true_labels[i], pred_labels_i)          \n",
        "                acc_set.append(acc)\n",
        "\n",
        "                f1 = f1_score(true_labels[i], pred_labels_i)\n",
        "                # print(f\"Acc: {acc}\\tF1: {f1}\")          \n",
        "                f1_set.append(f1)\n",
        "                #############################################################################\n",
        "                # Create a barplot showing the Accuracy score for each batch of test samples.\n",
        "        avgacc = np.mean(acc_set)\n",
        "        accuracy_dict[name].append(round(avgacc, 2))\n",
        "        avgf1 = np.mean(f1_set)\n",
        "        f1_dict[name].append(round(avgf1, 2))\n",
        "        print(f\"AVG Acc: {avgacc}\\tAVG F1: {avgf1}\")\n",
        "        scores = precision_recall_fscore_support(total_target, total_preds, average=None, labels=[0,1])\n",
        "        pr0, pr1 = scores[0]\n",
        "        re0, re1 = scores[1]\n",
        "        f10, f11 = scores[2]\n",
        "        total_acc = accuracy_score(total_target, total_preds)\n",
        "        total_f1 = f1_score(total_target, total_preds)\n",
        "        class1_dict_f1[name].append(f11)\n",
        "        class1_dict_pr[name].append(pr1)\n",
        "        class1_dict_re[name].append(re1)\n",
        "\n",
        "        class0_dict_f1[name].append(f10)\n",
        "        class0_dict_pr[name].append(pr0)\n",
        "        class0_dict_re[name].append(re0)\n",
        "        \n",
        "        # print(f\"Total Acc: {total_acc}\\tTotal F1: {total_f1}\")\n",
        "header = \"experiments,0,23,111,avg,std\\n\"\n",
        "output_acc = header[::]\n",
        "output_f1 = header[::]\n",
        "class1_acc = header[::]\n",
        "class1_f1 = header[::]\n",
        "class1_pr = header[::]\n",
        "class1_re = header[::]\n",
        "class0_acc = header[::]\n",
        "class0_f1 = header[::]\n",
        "class0_pr = header[::]\n",
        "class0_re = header[::]\n",
        "\n",
        "print(header)\n",
        "for key in accuracy_dict:\n",
        "    values = accuracy_dict[key]\n",
        "    values += [np.mean(values), np.std(values)]\n",
        "    values = \",\".join([str(elem) for elem in values])\n",
        "    output_acc += f\"{key},{values}\\n\"\n",
        "    print(key, \",\", values)\n",
        "print(\"\\n\\n\")\n",
        "print(header)\n",
        "for key in f1_dict:\n",
        "    values = f1_dict[key]\n",
        "    values += [np.mean(values), np.std(values)]\n",
        "    values = \",\".join([str(elem) for elem in values])\n",
        "    output_f1 += f\"{key},{values}\\n\"\n",
        "    print(key, \",\", values)\n",
        "\n",
        "\n",
        "# print(header)\n",
        "# for key in class1_dict_acc:\n",
        "#     values = class1_dict_acc[key]\n",
        "#     values += [np.mean(values), np.std(values)]\n",
        "#     values = \",\".join([str(elem) for elem in values])\n",
        "#     class1_acc += f\"{key},{values}\\n\"\n",
        "#     print(key, \",\", values)  \n",
        "print(header)\n",
        "for key in class1_dict_f1:\n",
        "    values = class1_dict_f1[key]\n",
        "    values += [np.mean(values), np.std(values)]\n",
        "    values = \",\".join([str(elem) for elem in values])\n",
        "    class1_f1 += f\"{key},{values}\\n\"\n",
        "    print(key, \",\", values)  \n",
        "print(header)\n",
        "for key in class1_dict_pr:\n",
        "    values = class1_dict_pr[key]\n",
        "    values += [np.mean(values), np.std(values)]\n",
        "    values = \",\".join([str(elem) for elem in values])\n",
        "    class1_pr += f\"{key},{values}\\n\"\n",
        "    print(key, \",\", values)\n",
        "print(header)\n",
        "for key in class1_dict_re:\n",
        "    values = class1_dict_re[key]\n",
        "    values += [np.mean(values), np.std(values)]\n",
        "    values = \",\".join([str(elem) for elem in values])\n",
        "    class1_re += f\"{key},{values}\\n\"\n",
        "    print(key, \",\", values)\n",
        "\n",
        "\n",
        "# print(header)\n",
        "# for key in class0_dict_acc:\n",
        "#     values = class0_dict_acc[key]\n",
        "#     values += [np.mean(values), np.std(values)]\n",
        "#     values = \",\".join([str(elem) for elem in values])\n",
        "#     class0_acc += f\"{key},{values}\\n\"\n",
        "#     print(key, \",\", values)  \n",
        "print(header)\n",
        "for key in class0_dict_f1:\n",
        "    values = class0_dict_f1[key]\n",
        "    values += [np.mean(values), np.std(values)]\n",
        "    values = \",\".join([str(elem) for elem in values])\n",
        "    class0_f1 += f\"{key},{values}\\n\"\n",
        "    print(key, \",\", values)  \n",
        "print(header)\n",
        "for key in class0_dict_pr:\n",
        "    values = class0_dict_pr[key]\n",
        "    values += [np.mean(values), np.std(values)]\n",
        "    values = \",\".join([str(elem) for elem in values])\n",
        "    class0_pr += f\"{key},{values}\\n\"\n",
        "    print(key, \",\", values)\n",
        "print(header)\n",
        "for key in class0_dict_re:\n",
        "    values = class0_dict_re[key]\n",
        "    values += [np.mean(values), np.std(values)]\n",
        "    values = \",\".join([str(elem) for elem in values])\n",
        "    class0_re += f\"{key},{values}\\n\"\n",
        "    print(key, \",\", values) \n",
        "with open(\"acc_results_8.csv\", \"w\") as fptr:\n",
        "    fptr.write(output_acc)\n",
        "    fptr.close()\n",
        "with open(\"f1_results_8.csv\", \"w\") as fptr:\n",
        "    fptr.write(output_f1)\n",
        "    fptr.close()\n",
        "\n",
        "# with open(\"class1_acc_results.csv\", \"w\") as fptr:\n",
        "#     fptr.write(class1_acc)\n",
        "#     fptr.close()\n",
        "with open(\"class1_f1_results_8.csv\", \"w\") as fptr:\n",
        "    fptr.write(class1_f1)\n",
        "    fptr.close()\n",
        "with open(\"class1_pr_results_8.csv\", \"w\") as fptr:\n",
        "    fptr.write(class1_pr)\n",
        "    fptr.close()\n",
        "with open(\"class1_re_results_8.csv\", \"w\") as fptr:\n",
        "    fptr.write(class1_re)\n",
        "    fptr.close()\n",
        "\n",
        "# with open(\"class0_acc_results.csv\", \"w\") as fptr:\n",
        "#     fptr.write(class0_acc)\n",
        "#     fptr.close()\n",
        "with open(\"class0_f1_results_8.csv\", \"w\") as fptr:\n",
        "    fptr.write(class0_f1)\n",
        "    fptr.close()\n",
        "with open(\"class0_pr_results_8.csv\", \"w\") as fptr:\n",
        "    fptr.write(class0_pr)\n",
        "    fptr.close()\n",
        "with open(\"class0_re_results_8.csv\", \"w\") as fptr:\n",
        "    fptr.write(class0_re)\n",
        "    fptr.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "########## transcript_with_tags2\n",
            "fold 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6fb0e6fcd4ca4d01a7d15443ef0b4637",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b50a8f44e1804c41adeeaf3584ba1fdf",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=28.0, style=ProgressStyle(description_w…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "300732dd250e4968918798260434b6e0",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=466062.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "df505c2b04164b02b71b4995cee56a9b",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=570.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d3ae222619b84f99ae7fa47a674685c8",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=440473133.0, style=ProgressStyle(descri…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "fold 2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "fold 3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "fold 4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "fold 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "AVG Acc: 0.767965367965368\tAVG F1: 0.7454168440255396\n",
            "########## transcript_without_repetition\n",
            "fold 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "fold 2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "fold 3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "fold 4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "fold 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "AVG Acc: 0.7506493506493506\tAVG F1: 0.7363809523809524\n",
            "########## transcript_without_retracing\n",
            "fold 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "fold 2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "fold 3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "fold 4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "fold 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "AVG Acc: 0.7783549783549784\tAVG F1: 0.7686153846153847\n",
            "########## transcript_without_either\n",
            "fold 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "fold 2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "fold 3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "fold 4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "fold 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "AVG Acc: 0.7878787878787878\tAVG F1: 0.7715320910973084\n",
            "########## transcript_with_tags2\n",
            "fold 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "fold 2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "fold 3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "fold 4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "fold 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "AVG Acc: 0.777922077922078\tAVG F1: 0.773809523809524\n",
            "########## transcript_without_repetition\n",
            "fold 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "fold 2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "fold 3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "fold 4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "fold 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "AVG Acc: 0.8056277056277056\tAVG F1: 0.7895906432748537\n",
            "########## transcript_without_retracing\n",
            "fold 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "fold 2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "fold 3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "fold 4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "fold 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "AVG Acc: 0.7597402597402597\tAVG F1: 0.7783656343656344\n",
            "########## transcript_without_either\n",
            "fold 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "fold 2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "fold 3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "fold 4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "fold 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "AVG Acc: 0.7575757575757576\tAVG F1: 0.7455488523680742\n",
            "########## transcript_with_tags2\n",
            "fold 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "fold 2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "fold 3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "fold 4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "fold 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "AVG Acc: 0.8051948051948052\tAVG F1: 0.798175114994337\n",
            "########## transcript_without_repetition\n",
            "fold 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "fold 2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "fold 3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "fold 4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "fold 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "AVG Acc: 0.8329004329004329\tAVG F1: 0.8302765284024654\n",
            "########## transcript_without_retracing\n",
            "fold 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "fold 2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "fold 3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "fold 4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "fold 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "AVG Acc: 0.8238095238095238\tAVG F1: 0.8256720659278203\n",
            "########## transcript_without_either\n",
            "fold 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "fold 2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "fold 3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "fold 4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "fold 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "AVG Acc: 0.8147186147186147\tAVG F1: 0.8099999999999999\n",
            "experiments,0,23,111,avg,std\n",
            "\n",
            "transcript_with_tags2 , 0.77,0.78,0.81,0.7866666666666667,0.016996731711975962\n",
            "transcript_without_repetition , 0.75,0.81,0.83,0.7966666666666667,0.033993463423951896\n",
            "transcript_without_retracing , 0.78,0.76,0.82,0.7866666666666666,0.024944382578492918\n",
            "transcript_without_either , 0.79,0.76,0.81,0.7866666666666667,0.020548046676563275\n",
            "\n",
            "\n",
            "\n",
            "experiments,0,23,111,avg,std\n",
            "\n",
            "transcript_with_tags2 , 0.75,0.77,0.8,0.7733333333333334,0.020548046676563275\n",
            "transcript_without_repetition , 0.74,0.79,0.83,0.7866666666666666,0.036817870057290855\n",
            "transcript_without_retracing , 0.77,0.78,0.83,0.7933333333333333,0.026246692913372675\n",
            "transcript_without_either , 0.77,0.75,0.81,0.7766666666666667,0.024944382578492966\n",
            "experiments,0,23,111,avg,std\n",
            "\n",
            "transcript_with_tags2 , 0.7572815533980582,0.7735849056603773,0.8037383177570094,0.778201592271815,0.019244792977616054\n",
            "transcript_without_repetition , 0.7522935779816514,0.7920792079207921,0.8363636363636364,0.7935788074220267,0.034337834120065185\n",
            "transcript_without_retracing , 0.7818181818181819,0.7758620689655172,0.8224299065420562,0.7933700524419184,0.020691788392490206\n",
            "transcript_without_either , 0.7889908256880734,0.7450980392156864,0.8076923076923077,0.7805937241986891,0.026234761883146582\n",
            "experiments,0,23,111,avg,std\n",
            "\n",
            "transcript_with_tags2 , 0.7959183673469388,0.7884615384615384,0.8113207547169812,0.7985668868418195,0.00951829584149553\n",
            "transcript_without_repetition , 0.7454545454545455,0.851063829787234,0.8214285714285714,0.8059823155567836,0.04447673811224783\n",
            "transcript_without_retracing , 0.7678571428571429,0.7258064516129032,0.8301886792452831,0.7746174245717764,0.0428811412940498\n",
            "transcript_without_either , 0.7818181818181819,0.7916666666666666,0.84,0.8044949494949495,0.025425769368593323\n",
            "experiments,0,23,111,avg,std\n",
            "\n",
            "transcript_with_tags2 , 0.7222222222222222,0.7592592592592593,0.7962962962962963,0.7592592592592592,0.0302406141084343\n",
            "transcript_without_repetition , 0.7592592592592593,0.7407407407407407,0.8518518518518519,0.7839506172839507,0.04860498687661613\n",
            "transcript_without_retracing , 0.7962962962962963,0.8333333333333334,0.8148148148148148,0.8148148148148149,0.01512030705421717\n",
            "transcript_without_either , 0.7962962962962963,0.7037037037037037,0.7777777777777778,0.7592592592592592,0.04000457221239419\n",
            "experiments,0,23,111,avg,std\n",
            "\n",
            "transcript_with_tags2 , 0.7787610619469028,0.7818181818181819,0.8073394495412846,0.7893062311021231,0.012812343551766222\n",
            "transcript_without_repetition , 0.7476635514018692,0.817391304347826,0.830188679245283,0.7984145116649928,0.0362646584320361\n",
            "transcript_without_retracing , 0.7735849056603773,0.74,0.8256880733944955,0.7797576596849575,0.035253261312027\n",
            "transcript_without_either , 0.7850467289719626,0.7719298245614035,0.8214285714285715,0.7928017083206459,0.02093858103882038\n",
            "experiments,0,23,111,avg,std\n",
            "\n",
            "transcript_with_tags2 , 0.7457627118644068,0.7678571428571429,0.8,0.7712066182405165,0.022268589164766154\n",
            "transcript_without_repetition , 0.7547169811320755,0.7704918032786885,0.8461538461538461,0.7904542101882034,0.0399086318020564\n",
            "transcript_without_retracing , 0.7884615384615384,0.8043478260869565,0.8181818181818182,0.8036637275767711,0.012142892289775862\n",
            "transcript_without_either , 0.7924528301886793,0.7333333333333333,0.7931034482758621,0.7729632039326249,0.028023809029243812\n",
            "experiments,0,23,111,avg,std\n",
            "\n",
            "transcript_with_tags2 , 0.8148148148148148,0.7962962962962963,0.8148148148148148,0.808641975308642,0.008729713347982055\n",
            "transcript_without_repetition , 0.7407407407407407,0.8703703703703703,0.8148148148148148,0.808641975308642,0.05310077325334956\n",
            "transcript_without_retracing , 0.7592592592592593,0.6851851851851852,0.8333333333333334,0.7592592592592594,0.0604812282168686\n",
            "transcript_without_either , 0.7777777777777778,0.8148148148148148,0.8518518518518519,0.8148148148148149,0.0302406141084343\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}